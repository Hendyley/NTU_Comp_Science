{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hendyley/Pole_Dancer/blob/main/SC3000_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZkbK80ft0RV"
   },
   "source": [
    "# <h1><center>**SC3000 Assignment 1 Balancing a Pole on a Cart**<center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/694/1*iOceBp5ks4X_Bs-8vqT2qA.png\" href=\"https://miro.medium.com/max/694/1*iOceBp5ks4X_Bs-8vqT2qA.png\"></center>\n",
    "\n",
    "\n",
    "##### Source: https://miro.medium.com/max/694/1*iOceBp5ks4X_Bs-8vqT2qA.png\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "<h2><center><strong>Lab Group : A42<strong><center></h2>\n",
    "<h2><center>Group Name: <strong>PoleDancer</strong><center></h2>\n",
    "<ul>\n",
    "<li>Brendon Tan : Task 1, 2, 3, 4</li>\n",
    "<li>Hendy : Task 1, 2, 3, 4</li>\n",
    "<li>Wong Ri Hong : Task 1, 2, 3, 4</li>\n",
    "</ul>\n",
    "\n",
    "<hr></hr>\n",
    "\n",
    "\n",
    "Reference: https://medium.com/swlh/using-q-learning-for-openais-cartpole-v1-4a216ef237df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBiYOoesYMvr"
   },
   "source": [
    "### Installing dependencies:\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRl0Yr9pAHFY"
   },
   "source": [
    "###### Reference:\n",
    "###### Python Gymnasium source: https://gymnasium.farama.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45757,
     "status": "ok",
     "timestamp": 1679467636261,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "xjdARNrQAuG9",
    "outputId": "7f55ac3b-ee81-4d38-c678-9b63a7131f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (67.6.1)\n",
      "Requirement already satisfied: scores in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.24.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (4.5.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Collecting Box2D\n",
      "  Using cached Box2D-2.3.2.tar.gz (427 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: Box2D\n",
      "  Running setup.py install for Box2D: started\n",
      "  Running setup.py install for Box2D: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Box2D is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for Box2D did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [18 lines of output]\n",
      "  Using setuptools (version 67.6.1).\n",
      "  running install\n",
      "  C:\\Users\\Hendy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\Box2D\n",
      "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\n",
      "  creating build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-311\\Box2D\\b2\n",
      "  running build_ext\n",
      "  building 'Box2D._Box2D' extension\n",
      "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "  error: command 'swig.exe' failed: None\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "Box2D\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tools in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.9)\n",
      "Requirement already satisfied: pytils in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tools) (0.4.1)\n",
      "Requirement already satisfied: six in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tools) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tools) (4.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install --upgrade setuptools\n",
    "!pip install scores\n",
    "!pip install gymnasium\n",
    "!pip install Box2D\n",
    "!pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55773,
     "status": "ok",
     "timestamp": 1679467692030,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "PbgnVwZmX5uW",
    "outputId": "2ec38841-3cb4-4899-fb93-1ed824a54392",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: gym in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym[classic_control]) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gym[classic_control]) (0.0.8)\n",
      "Collecting pygame==2.1.0\n",
      "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [77 lines of output]\n",
      "  \n",
      "  \n",
      "  WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  Using WINDOWS configuration...\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\setup.py\", line 388, in <module>\n",
      "      buildconfig.config.main(AUTO_CONFIG)\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\buildconfig\\config.py\", line 234, in main\n",
      "      deps = CFG.main(**kwds)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\buildconfig\\config_win.py\", line 511, in main\n",
      "      return setup_prebuilt_sdl2(prebuilt_dir)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\buildconfig\\config_win.py\", line 471, in setup_prebuilt_sdl2\n",
      "      DEPS.configure()\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\buildconfig\\config_win.py\", line 336, in configure\n",
      "      from . import vstools\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Temp\\pip-install-dj3bwy1g\\pygame_e35891ef1c6d4c348203ef0e1009d774\\buildconfig\\vstools.py\", line 11, in <module>\n",
      "      compiler.initialize()\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\", line 403, in initialize\n",
      "      vc_env = query_vcvarsall(VERSION, plat_spec)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\Hendy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\", line 281, in query_vcvarsall\n",
      "      raise DistutilsPlatformError(\"Unable to find vcvarsall.bat\")\n",
      "  distutils.errors.DistutilsPlatformError: Unable to find vcvarsall.bat\n",
      "  Making dir :prebuilt_downloads:\n",
      "  Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.16-VC.zip 13d952c333f3c2ebe9b7bc0075b4ad2f784e7584\n",
      "  Unzipping :prebuilt_downloads\\SDL2-devel-2.0.16-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_image/release/SDL2_image-devel-2.0.5-VC.zip 137f86474691f4e12e76e07d58d5920c8d844d5b\n",
      "  Unzipping :prebuilt_downloads\\SDL2_image-devel-2.0.5-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_ttf/release/SDL2_ttf-devel-2.0.15-VC.zip 1436df41ebc47ac36e02ec9bda5699e80ff9bd27\n",
      "  Unzipping :prebuilt_downloads\\SDL2_ttf-devel-2.0.15-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_mixer/release/SDL2_mixer-devel-2.0.4-VC.zip 9097148f4529cf19f805ccd007618dec280f0ecc\n",
      "  Unzipping :prebuilt_downloads\\SDL2_mixer-devel-2.0.4-VC.zip:\n",
      "  Downloading... https://www.pygame.org/ftp/jpegsr9d.zip ed10aa2b5a0fcfe74f8a6f7611aeb346b06a1f99\n",
      "  Unzipping :prebuilt_downloads\\jpegsr9d.zip:\n",
      "  Downloading... https://pygame.org/ftp/prebuilt-x64-pygame-1.9.2-20150922.zip 3a5af3427b3aa13a0aaf5c4cb08daaed341613ed\n",
      "  Unzipping :prebuilt_downloads\\prebuilt-x64-pygame-1.9.2-20150922.zip:\n",
      "  copying into .\\prebuilt-x64\n",
      "  Path for SDL: prebuilt-x64\\SDL2-2.0.16\n",
      "  ...Library directory for SDL: prebuilt-x64/SDL2-2.0.16/lib/x64\n",
      "  ...Include directory for SDL: prebuilt-x64/SDL2-2.0.16/include\n",
      "  Path for FONT: prebuilt-x64\\SDL2_ttf-2.0.15\n",
      "  ...Library directory for FONT: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64\n",
      "  ...Include directory for FONT: prebuilt-x64/SDL2_ttf-2.0.15/include\n",
      "  Path for IMAGE: prebuilt-x64\\SDL2_image-2.0.5\n",
      "  ...Library directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/lib/x64\n",
      "  ...Include directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/include\n",
      "  Path for MIXER: prebuilt-x64\\SDL2_mixer-2.0.4\n",
      "  ...Library directory for MIXER: prebuilt-x64/SDL2_mixer-2.0.4/lib/x64\n",
      "  ...Include directory for MIXER: prebuilt-x64/SDL2_mixer-2.0.4/include\n",
      "  Path for PORTMIDI: prebuilt-x64\n",
      "  ...Library directory for PORTMIDI: prebuilt-x64/lib\n",
      "  ...Include directory for PORTMIDI: prebuilt-x64/include\n",
      "  DLL for SDL2: prebuilt-x64/SDL2-2.0.16/lib/x64/SDL2.dll\n",
      "  DLL for SDL2_ttf: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64/SDL2_ttf.dll\n",
      "  DLL for SDL2_image: prebuilt-x64/SDL2_image-2.0.5/lib/x64/SDL2_image.dll\n",
      "  DLL for SDL2_mixer: prebuilt-x64/SDL2_mixer-2.0.4/lib/x64/SDL2_mixer.dll\n",
      "  DLL for portmidi: prebuilt-x64/lib/portmidi.dll\n",
      "  Path for FREETYPE not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  Path for PNG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  Path for JPEG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  DLL for freetype: prebuilt-x64/SDL2_ttf-2.0.15/lib/x64/libfreetype-6.dll\n",
      "  \n",
      "  ---\n",
      "  For help with compilation see:\n",
      "      https://www.pygame.org/wiki/CompileWindows\n",
      "  To contribute to pygame development see:\n",
      "      https://www.pygame.org/contribute.html\n",
      "  ---\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "The system cannot find the path specified.\n",
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (67.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!pip install gym pyvirtualdisplay\n",
    "!pip install gym pyvirtualdisplay\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install gym[classic_control]\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwKbYeTgbaTA"
   },
   "source": [
    "## Importing dependencies and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1679468217182,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "j6KpgCLGYWmj"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "gymlogger.set_level(40) #error only\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "  mp4list = glob.glob('video/*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hendy\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehbqP9CXbmo7"
   },
   "source": [
    "## Loading CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1679467839247,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "Go12dH4qbwBy"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XZ9g3xrcAXE"
   },
   "source": [
    "We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1679467841403,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "ytxvVmLdcRyw",
    "outputId": "35e819e1-0abe-4bd0-a8d1-9835c931c5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVXGWi_Ncfg-"
   },
   "source": [
    "The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1679467842827,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "DyqHr9I5cdkX",
    "outputId": "3b78a33c-aea0-46a8-dbbc-e4313d603664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFOdaU2Gdyg0"
   },
   "source": [
    "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679467844646,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "VMr6qAqxdOsm",
    "outputId": "b6e48e8f-541d-4622-b56d-33e9ed27d1f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observations: (array([-0.03510504,  0.0276076 ,  0.01148383,  0.01843208], dtype=float32), {})\n",
      "Action Space Discrete(2)\n",
      "State Space Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)\n",
    "\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnG2QdfbeZrI"
   },
   "source": [
    "For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679467848123,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "MmfMDvyYdWGk",
    "outputId": "f0d536ea-3518-4a67-89e9-073d54df5774"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hendy\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#observation, reward, done, info, terminated, truncated = env.step(0)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew observations after choosing action 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, observation)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "observation, reward, done, info = env.step(0)\n",
    "#observation, reward, done, info, terminated, truncated = env.step(0)\n",
    "print(\"New observations after choosing action 0:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj0zCh59fhBb"
   },
   "source": [
    "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1679467854577,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "AVucQVRwf6Jm",
    "outputId": "caea0623-eff4-4db1-af15-3ba0e4cf02d8"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done, info = env.step(0)\n",
    "    cumulative_reward += reward\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oIzK9SzhlWN"
   },
   "source": [
    "## Task 1: Development of an RL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cc6_e5c_huiq"
   },
   "source": [
    "An example of a naive agent is given below, which randomly chooses an action regardless of the observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 703,
     "status": "ok",
     "timestamp": 1679467858460,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "Hk-M4QEfh6l5"
   },
   "outputs": [],
   "source": [
    "def rand_policy_agent(observation):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8fMCGmW1qZu"
   },
   "source": [
    "## Initialize parameters for Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1679467861432,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "iMoBoR5KwrPJ",
    "outputId": "4670ad8a-23c6-413a-c1ce-b64898b8f265",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 40000\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n",
    "\n",
    "\n",
    "Observation = [30, 30, 100, 100]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "epsilon = 1\n",
    "epsilon_decay_value = 0.9999\n",
    "\n",
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,15,50,50])\n",
    "    return tuple(discrete_state.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvNE_nOKRpmM"
   },
   "source": [
    "This is our Q-learning model. \n",
    "We set our learning rate to be low at 0.1 because we do not want the agent to overreact to new information which will cause it to overwrite its old information. \n",
    "Our discount rate is 0.95 to accommodate long term rewards.\n",
    "We determine the number of episodes of 40000 to be sufficient for the exploration (not too low/high) and exploitation for the Q-learning agent.\n",
    "We initialize total reward and prior reward to 0.\n",
    "\n",
    "We set the observation space to be [30, 30, 100, 100] as the first 2 variables (Cartpole position and cartpole velocity) are not as important as the next 2 variables (Pole Angle and pole velocity). It is unlikely that the cartpole will end up at the extreme ends of the boundary (edge of the display) and the velocity is bounded by a small range. Whereas for the pole, when it topples, it will definitely reach the extreme angles.\n",
    "\n",
    "Similarly, we set the window size to be [0.25, 0.25, 0.01, 0.1] for a similar reason, for more precision on pole angle and velocity as these 2 factors are more important then cartpole position and velocity.\n",
    "\n",
    "Initialisation of epsilon to 1 is because there is no information to exploit initially.\n",
    "We determine that the epsilon decay value of 0.9999 is sufficient for our model’s exploitation.\n",
    "\n",
    "We initialize the Q-table with random values (0 or 1) for each state-action pair in the environment since the agent has no knowledge initially of which action to take to maximize the reward.\n",
    "\n",
    "We define a function get_discrete_state to classify each state into a discrete state based on which window it fits into. We add [15,15,50,50] to the discrete array to avoid negative indexing in our Q-table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ps4cPki11Ce"
   },
   "source": [
    "## Q learning, train the agent on 40000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 209373,
     "status": "ok",
     "timestamp": 1679468075161,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "fIb9KVpE4lsI",
    "outputId": "3c8943da-9215-4e15-b418-a970969d5084"
   },
   "outputs": [],
   "source": [
    "for episode in range(EPISODES + 1): \n",
    "    \n",
    "    #Initializing starting state for each epsiode in the given environment range\n",
    "     \n",
    "    discrete_state = get_discrete_state(env.reset())   \n",
    "    done = False\n",
    "    episode_reward = 0 \n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "\n",
    "    while not done: \n",
    "\n",
    "        if np.random.random() > epsilon:\n",
    "\n",
    "            action = np.argmax(q_table[discrete_state]) # exploit\n",
    "        else:\n",
    "\n",
    "            action = np.random.randint(0, env.action_space.n) #explore\n",
    "\n",
    "        new_state, reward, done, info = env.step(action) #get new state and reward after taking the action above\n",
    "        episode_reward += reward # add reward to current episode total reward\n",
    "        new_discrete_state = get_discrete_state(new_state) # discretize the new state\n",
    "           \n",
    "        if not done: \n",
    "            max_future_q = np.max(q_table[new_discrete_state]) #get max reward from q table\n",
    "            \n",
    "            # Using bellman equation to calculate new q-value\n",
    "            \n",
    "            current_q = q_table[discrete_state + (action,)] \n",
    "\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_discrete_state # Replace current state with the new discrete state\n",
    "\n",
    "    if epsilon > 0.05:  # Prevent epsilon from dropping too low, so that the model will keep exploring even at high episodes\n",
    "        if episode_reward > prior_reward and episode > 10000: # If episode reward is greater than previous episode reward, it means that our model is improving, so we should exploit more\n",
    "                                                              #Set to 10000 for sufficient exploration before exploitation begins\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000) #Decreasing Epsilon value so that the model will exploit more\n",
    "\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "\n",
    "    # For checking mean reward after every 2000 episodes                \n",
    "    total_reward += episode_reward \n",
    "    prior_reward = episode_reward # Save a record of the current episode reward to be used for comparison with future episode rewards\n",
    "\n",
    "    if episode % 1000 == 0: \n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "#print(q_table)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXt6rPoxJvR7"
   },
   "source": [
    "For each episode, we initialize the variables and environment.\n",
    "\n",
    "For each episode, while it is not done(out of bounds/pole tilted past angle) or truncated (episode length greater than 500), we generate a random value. If the value is greater than epsilon, the agent will exploit the values in the Q-table, otherwise it will explore and choose a random action.\n",
    "\n",
    "After taking the action, we get the values of the new state and increment the episode reward by, then we discretize the new state.\n",
    "\n",
    "If the new state is valid, we calculate the new Q-value for the state-action pair using the Bellman equation. Afterwards, we replace the old discrete state with the new one.\n",
    "\n",
    "We set the minimum value of epsilon at 0.05, so that the agent will always be able to explore even at higher episodes and not drop close to 0 due to the decay. In order to decrement epsilon, we check 2 conditions: If the current episode’s reward is greater than the previous, meaning the model is improving and the agent should exploit more; If the episode is greater than 10000, to ensure the agent has explored sufficiently initially before beginning to exploit, to avoid convergence to a suboptimal policy.\n",
    "\n",
    "Before the end of each episode, we save a record of the current episode’s reward so it can be compared against in the next episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmqmO59gImWZ"
   },
   "source": [
    "# Reinforcment Learning agent based on the Q-values we obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1679468086321,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "8bjZnmbjT0Hd"
   },
   "outputs": [],
   "source": [
    "def Reinforcement_Learning_agent(state):\n",
    "    discrete_state=get_discrete_state(state)\n",
    "    action = np.argmax(q_table[discrete_state]) \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAi7KKwNiegR"
   },
   "source": [
    "For Task 1, we can show the observation and chosen action below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1679468089669,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "ae2ia-vUiNKJ",
    "outputId": "6cfda084-4a84-420e-a42d-f34a162db67c"
   },
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "action = Reinforcement_Learning_agent(observation) #need to include [0] bcuz jupyter weird, colab noneed\n",
    "print(\"Observation:\", observation)\n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XtIQ0Rti1gm"
   },
   "source": [
    "## Task 2: Demonstrate the effectiveness of the RL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djBEShf0kGI4"
   },
   "source": [
    "For this task, use the agent developed in Task 1 to play the game for 100 episodes (refer to tutorial for how to play a round), record the cumulative reward for each round, and plot the reward for each round. A sample plotting code is given below. Note that you must include code to play for 100 episodes and use the code to obtain round_results for plotting. DO NOT record the round results in advance and paste the results to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2739,
     "status": "ok",
     "timestamp": 1679468116179,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "xpwqe5jRxd9J"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "test_episodes=100\n",
    "episode_results = []\n",
    "\n",
    "for test_episode in range(test_episodes):\n",
    "    initial_state=env.reset()\n",
    "    state=initial_state[0]\n",
    "    cumulative_reward = 0\n",
    "    done=False\n",
    "    while not done:\n",
    "        action = Reinforcement_Learning_agent(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        #env.render()\n",
    "    episode_results.append(cumulative_reward)\n",
    "  #print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1679468120413,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "RZrCKywQi6CE",
    "outputId": "a575e959-db56-44d2-82d9-aef64ccb4d10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_results)\n",
    "plt.title('Cumulative reward for each episode')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.xlabel('episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679468124037,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "ocGqOrVbuXbd",
    "outputId": "22f25c38-e5cc-4f8f-9207-087fdee107b6"
   },
   "outputs": [],
   "source": [
    "episode_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XndSYH7wlvn7"
   },
   "source": [
    "Print the average reward over the 100 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679468130656,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "VXkXH7YUzK82",
    "outputId": "99a7c53f-4f8f-4d2e-c4e8-993fd8cb5e0e"
   },
   "outputs": [],
   "source": [
    "print(\"Average reward: \", sum(episode_results) / len(episode_results))\n",
    "print(\"Is my agent good enough?\", sum(episode_results) / len(episode_results) > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg0DCT38lFA6"
   },
   "source": [
    "## Task 3: Render one episode played by the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx1awMr9lc_w"
   },
   "source": [
    "Plug your agent to the code below to obtain rendered result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5917,
     "status": "ok",
     "timestamp": 1679468143544,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "eB7Rb-4IRpmO",
    "outputId": "63f871fa-c28e-4b30-ae8b-740f202c260a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 3433,
     "status": "ok",
     "timestamp": 1679468351353,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "LYyavfbIa47D",
    "outputId": "e3bb3b67-481d-44e4-9c40-8833f78f809a"
   },
   "outputs": [],
   "source": [
    "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
    "print(env)\n",
    "initial_observation = env.reset()\n",
    "observation=initial_observation\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    env.render()\n",
    "    #your agent goes here\n",
    "    action = Reinforcement_Learning_agent(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done: \n",
    "        break;    \n",
    "env.close()\n",
    "show_video()\n",
    "if i==500:\n",
    "  print(\"Im alive\")\n",
    "else:\n",
    "  print(\"Die\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCHm4u6q7gei"
   },
   "source": [
    "## Task 4: Format the Jupyter notebook by including step-by-step instruction and explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1679467710550,
     "user": {
      "displayName": "Brendon Tan",
      "userId": "18329209753696780422"
     },
     "user_tz": -480
    },
    "id": "XrRQDsev7qtn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
