{"cells":[{"cell_type":"code","execution_count":null,"id":"c8c630ad","metadata":{"id":"c8c630ad"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e54b33d1","metadata":{"id":"e54b33d1","outputId":"099e918b-d082-46f4-d30a-6575bc212d54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730515702076,"user_tz":-480,"elapsed":8050,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"markdown","id":"a3a78089","metadata":{"id":"a3a78089"},"source":["## Part 1"]},{"cell_type":"code","execution_count":null,"id":"98bbf012","metadata":{"id":"98bbf012","outputId":"b3aea936-452a-4f9d-d57b-2b30d6eb20f4","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"error","timestamp":1730831477965,"user_tz":-480,"elapsed":340,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fda3975c9c17>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Question 1(a): What is the size of the vocabulary formed from your training data?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Question 1(a): What is the size of the vocabulary formed from your training data?\n","\n","from datasets import load_dataset\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","\n","# Initialize an empty set to store unique words\n","vocabulary = set()\n","\n","# Tokenize each review in the training dataset and update the vocabulary set\n","for text in train_dataset['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Print the size of the vocabulary\n","print(\"The size of the vocabulary is:\", len(vocabulary))"]},{"cell_type":"code","execution_count":null,"id":"b3f3bedd","metadata":{"id":"b3f3bedd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"edb13d4a","metadata":{"id":"edb13d4a","outputId":"4f07a375-a279-47e0-8879-a389589a7522","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1730831474743,"user_tz":-480,"elapsed":1081,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'vocabulary' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cb557e76d305>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Identify OOV words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0moov_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mglove_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Print the number of OOV words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"]}],"source":["# Question 1(b): How many OOV words exist in your training data?\n","\n","import numpy as np\n","\n","# Load the GloVe embeddings (make sure to download 'glove.6B.100d.txt' and place it in the working directory)\n","glove_vocab = set()\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        word = line.split()[0]\n","        glove_vocab.add(word)\n","\n","# Identify OOV words\n","oov_words = vocabulary - glove_vocab\n","\n","# Print the number of OOV words\n","print(\"Number of OOV words in the training data:\", len(oov_words))"]},{"cell_type":"code","execution_count":null,"id":"8b92c546","metadata":{"id":"8b92c546"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"50688a71","metadata":{"id":"50688a71","outputId":"5c786541-651b-4f26-e884-54cc7672fff8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730515805770,"user_tz":-480,"elapsed":3937,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding matrix shape: (18029, 100)\n"]}],"source":["# Question 1(c): Mitigating OOV Words by Initializing Random Embeddings\n","\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","import numpy as np\n","\n","# Create mappings between words and indices\n","word2idx = {}\n","idx2word = {}\n","for idx, word in enumerate(vocabulary):\n","    word2idx[word] = idx\n","    idx2word[idx] = word\n","\n","# Initialize the embedding matrix with random values\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocabulary), embedding_dim))\n","\n","# Load GloVe embeddings into the embedding matrix where possible\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        glove_word = values[0]\n","        if glove_word in word2idx:\n","            vector = np.asarray(values[1:], dtype='float32')\n","            idx = word2idx[glove_word]\n","            embedding_matrix[idx] = vector\n","\n","# Now, embedding_matrix contains GloVe embeddings for known words and random values for OOV words\n","print(\"Embedding matrix shape:\", embedding_matrix.shape)"]},{"cell_type":"markdown","id":"c8eba559","metadata":{"id":"c8eba559"},"source":["## Part 2"]},{"cell_type":"code","execution_count":null,"id":"bff4efb2","metadata":{"id":"bff4efb2","outputId":"5c5ba6c7-6c83-4ec5-c428-4db22b5eed49"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Samuel\n","[nltk_data]     Ng\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","C:\\Users\\Samuel Ng\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6354, Validation Accuracy: 0.6979\n","Epoch [2/30], Loss: 0.5430, Validation Accuracy: 0.7026\n","Epoch [3/30], Loss: 0.5229, Validation Accuracy: 0.7364\n","Epoch [4/30], Loss: 0.5023, Validation Accuracy: 0.7598\n","Epoch [5/30], Loss: 0.4745, Validation Accuracy: 0.7364\n","Epoch [6/30], Loss: 0.4521, Validation Accuracy: 0.7580\n","Epoch [7/30], Loss: 0.4301, Validation Accuracy: 0.7392\n","Epoch [8/30], Loss: 0.3914, Validation Accuracy: 0.7655\n","Epoch [9/30], Loss: 0.3673, Validation Accuracy: 0.7711\n","Epoch [10/30], Loss: 0.3432, Validation Accuracy: 0.7767\n","Epoch [11/30], Loss: 0.3233, Validation Accuracy: 0.7692\n","Epoch [12/30], Loss: 0.3010, Validation Accuracy: 0.7767\n","Epoch [13/30], Loss: 0.2818, Validation Accuracy: 0.7795\n","Epoch [14/30], Loss: 0.2546, Validation Accuracy: 0.7805\n","Epoch [15/30], Loss: 0.2361, Validation Accuracy: 0.7786\n","Epoch [16/30], Loss: 0.2158, Validation Accuracy: 0.7730\n","Epoch [17/30], Loss: 0.1837, Validation Accuracy: 0.7467\n","Epoch [18/30], Loss: 0.1455, Validation Accuracy: 0.7636\n","Epoch [19/30], Loss: 0.1194, Validation Accuracy: 0.7711\n","Early stopping!\n","Test Accuracy: 0.7777\n"]}],"source":["# Import necessary libraries\n","from datasets import load_dataset\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn as nn\n","\n","# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = 'glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)\n","\n","# Define a function to convert sentences to indices\n","def sentence_to_indices(sentence, word2idx):\n","    tokens = word_tokenize(sentence.lower())\n","    indices = []\n","    for token in tokens:\n","        if token in word2idx:\n","            indices.append(word2idx[token])\n","        else:\n","            indices.append(word2idx['<unk>'])  # Map unknown words to '<unk>'\n","    return indices\n","\n","# Define the custom Dataset class\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, word2idx):\n","        self.texts = texts\n","        self.labels = labels\n","        self.word2idx = word2idx\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        indices = sentence_to_indices(text, self.word2idx)\n","        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","\n","# Create datasets for training, validation, and testing\n","train_dataset = SentimentDataset(train_raw['text'], train_raw['label'], word2idx)\n","val_dataset = SentimentDataset(validation_raw['text'], validation_raw['label'], word2idx)\n","test_dataset = SentimentDataset(test_raw['text'], test_raw['label'], word2idx)\n","\n","# Define the collate_fn function for padding within batches\n","def collate_fn(batch):\n","    sequences = [item[0] for item in batch]\n","    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n","    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=word2idx['<pad>'])\n","    return sequences_padded, labels\n","\n","# Create DataLoaders for training, validation, and testing\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","# Define the RNN model using pre-trained embeddings\n","class SentimentRNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n","# Instantiate the model\n","model = SentimentRNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","\n","\n","# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"d25d2ba3-a6df-4a02-905d-ca10c4b6f7c2","metadata":{"id":"d25d2ba3-a6df-4a02-905d-ca10c4b6f7c2"},"outputs":[],"source":["#3.1"]},{"cell_type":"code","execution_count":null,"id":"648ea703-d9ac-4256-8862-a5d6231fbbd6","metadata":{"id":"648ea703-d9ac-4256-8862-a5d6231fbbd6"},"outputs":[],"source":["class SentimentRNN_UpdateEmbeddings(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN_UpdateEmbeddings, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True  # Update the embeddings during training\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"id":"7845f038-3527-47fb-aa91-18912dab8858","metadata":{"id":"7845f038-3527-47fb-aa91-18912dab8858","outputId":"ad01e954-9bc7-489a-ef1e-dd6303654be0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6158, Validation Accuracy: 0.7223\n","Epoch [2/30], Loss: 0.4818, Validation Accuracy: 0.7608\n","Epoch [3/30], Loss: 0.3559, Validation Accuracy: 0.7552\n","Epoch [4/30], Loss: 0.2421, Validation Accuracy: 0.7645\n","Epoch [5/30], Loss: 0.1518, Validation Accuracy: 0.7608\n","Epoch [6/30], Loss: 0.0922, Validation Accuracy: 0.7655\n","Epoch [7/30], Loss: 0.0494, Validation Accuracy: 0.7664\n","Epoch [8/30], Loss: 0.0263, Validation Accuracy: 0.7542\n","Epoch [9/30], Loss: 0.0151, Validation Accuracy: 0.7570\n","Epoch [10/30], Loss: 0.0092, Validation Accuracy: 0.7448\n","Epoch [11/30], Loss: 0.0038, Validation Accuracy: 0.7477\n","Epoch [12/30], Loss: 0.0013, Validation Accuracy: 0.7448\n","Early stopping!\n","Test Accuracy: 0.7617\n"]}],"source":["# Instantiate the model\n","model = SentimentRNN_UpdateEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model_update_embeddings.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model_update_embeddings.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"4f4871eb-ca38-4b34-aeb5-71f8681c099f","metadata":{"id":"4f4871eb-ca38-4b34-aeb5-71f8681c099f"},"outputs":[],"source":["#3.2"]},{"cell_type":"code","execution_count":null,"id":"f4a04cf4-0aa3-4466-bbb3-677bf4d75aed","metadata":{"id":"f4a04cf4-0aa3-4466-bbb3-677bf4d75aed"},"outputs":[],"source":["# Load the GloVe embeddings\n","glove_vocab = set()\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        word = line.split()[0]\n","        glove_vocab.add(word)\n","\n","# Initialize the embedding matrix with random values\n","embedding_dim = 100\n","vocab_size = len(vocabulary)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Create a dictionary to store the GloVe embeddings\n","glove_embeddings = {}\n","glove_file = 'glove.6B.100d.txt'\n","with open(glove_file, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        glove_word = values[0]\n","        vector = np.asarray(values[1:], dtype='float32')\n","        # Normalize the loaded vectors to have unit norm\n","        vector /= np.linalg.norm(vector)\n","        glove_embeddings[glove_word] = vector\n","\n","# Load GloVe embeddings into the embedding matrix where possible\n","for idx, word in enumerate(vocabulary):\n","    if word in glove_embeddings:\n","        embedding_matrix[idx] = glove_embeddings[word]"]},{"cell_type":"code","execution_count":null,"id":"6a2a4400-44f8-40a6-93f3-683c5dc8df9a","metadata":{"id":"6a2a4400-44f8-40a6-93f3-683c5dc8df9a"},"outputs":[],"source":["class SentimentRNN_OOV(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN_OOV, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"id":"893eaca0-ee17-4434-a4c3-014ddcc1e452","metadata":{"id":"893eaca0-ee17-4434-a4c3-014ddcc1e452","outputId":"f491e627-396f-4ce4-9dd9-2a075636203f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6444, Validation Accuracy: 0.7514\n","Epoch [2/30], Loss: 0.4196, Validation Accuracy: 0.7552\n","Epoch [3/30], Loss: 0.2322, Validation Accuracy: 0.7495\n","Epoch [4/30], Loss: 0.1236, Validation Accuracy: 0.7542\n","Epoch [5/30], Loss: 0.0663, Validation Accuracy: 0.7392\n","Epoch [6/30], Loss: 0.0291, Validation Accuracy: 0.7458\n","Epoch [7/30], Loss: 0.0119, Validation Accuracy: 0.7477\n","Early stopping!\n"]}],"source":["\n","# Instantiate the model\n","model = SentimentRNN_OOV(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model_oov.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n"]},{"cell_type":"code","execution_count":null,"id":"5cb26e55-c0b7-4341-accd-7693a04c12ed","metadata":{"id":"5cb26e55-c0b7-4341-accd-7693a04c12ed","outputId":"67628f37-0b67-43b7-bd22-29cea3a64bc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.7720\n"]}],"source":["# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model_oov.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"ec35fbe0-eaf1-446e-ae13-38be9a9d9619","metadata":{"id":"ec35fbe0-eaf1-446e-ae13-38be9a9d9619"},"outputs":[],"source":["# biLSTM Model\n","class SentimentBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"id":"66822818-6b88-4c9a-a24b-d3e2521c7976","metadata":{"id":"66822818-6b88-4c9a-a24b-d3e2521c7976"},"outputs":[],"source":["# biLSTM Model\n","class SentimentBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n","# biGRU Model\n","class SentimentBiGRU(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiGRU, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.gru = nn.GRU(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        gru_out, h_n = self.gru(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"8b85e41b-0b7c-4a92-a2e4-35b26a260753","metadata":{"id":"8b85e41b-0b7c-4a92-a2e4-35b26a260753"},"outputs":[],"source":["# Instantiate the models\n","bilstm_model = SentimentBiLSTM(embedding_matrix)\n","bigru_model = SentimentBiGRU(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","bilstm_model.to(device)\n","bigru_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","bilstm_optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=0.001, weight_decay=1e-5)\n","bigru_optimizer = torch.optim.Adam(bigru_model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(bilstm_optimizer, 'max', patience=2, factor=0.5, verbose=True)"]},{"cell_type":"code","execution_count":null,"id":"97491727-7b18-4358-8347-f5948c55e741","metadata":{"id":"97491727-7b18-4358-8347-f5948c55e741","outputId":"9c24b704-dec7-42d8-9b2c-dbad2c8533ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6494, Validation Accuracy: 0.7345\n","Epoch [2/30], Loss: 0.4284, Validation Accuracy: 0.7523\n","Epoch [3/30], Loss: 0.2194, Validation Accuracy: 0.7411\n","Epoch [4/30], Loss: 0.1183, Validation Accuracy: 0.7570\n","Epoch [5/30], Loss: 0.0712, Validation Accuracy: 0.7683\n","Epoch [6/30], Loss: 0.0434, Validation Accuracy: 0.7439\n","Epoch [7/30], Loss: 0.0275, Validation Accuracy: 0.7580\n","Epoch [8/30], Loss: 0.0184, Validation Accuracy: 0.7561\n","Epoch [9/30], Loss: 0.0102, Validation Accuracy: 0.7561\n","Epoch [10/30], Loss: 0.0059, Validation Accuracy: 0.7552\n","Early stopping!\n","biLSTM Test Accuracy: 0.7795\n"]}],"source":["# Training loop with validation and early stopping for biLSTM\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    bilstm_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        bilstm_optimizer.zero_grad()\n","        outputs = bilstm_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(bilstm_model.parameters(), max_norm=1)  # Gradient clipping\n","        bilstm_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    bilstm_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = bilstm_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(bilstm_model.state_dict(), 'best_bilstm_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set for biLSTM\n","bilstm_model.load_state_dict(torch.load('best_bilstm_model.pt'))\n","bilstm_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = bilstm_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","bilstm_test_accuracy = correct / total\n","print(f'biLSTM Test Accuracy: {bilstm_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"66d3c019-9c12-4dd9-9346-d66f1f9831a0","metadata":{"id":"66d3c019-9c12-4dd9-9346-d66f1f9831a0","outputId":"0f69aa72-897f-41b5-90cb-bd3d7929dc0c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6209, Validation Accuracy: 0.7176\n","Epoch [2/30], Loss: 0.3693, Validation Accuracy: 0.7523\n","Epoch [3/30], Loss: 0.1926, Validation Accuracy: 0.7580\n","Epoch [4/30], Loss: 0.0981, Validation Accuracy: 0.7486\n","Epoch [5/30], Loss: 0.0543, Validation Accuracy: 0.7439\n","Epoch [6/30], Loss: 0.0264, Validation Accuracy: 0.7289\n","Epoch [7/30], Loss: 0.0156, Validation Accuracy: 0.7355\n","Epoch [8/30], Loss: 0.0113, Validation Accuracy: 0.7383\n","Early stopping!\n","biGRU Test Accuracy: 0.7795\n"]}],"source":["# Training loop with validation and early stopping for biGRU\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    bigru_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        bigru_optimizer.zero_grad()\n","        outputs = bigru_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(bigru_model.parameters(), max_norm=1)  # Gradient clipping\n","        bigru_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    bigru_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = bigru_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(bigru_model.state_dict(), 'best_bigru_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set for biGRU\n","bigru_model.load_state_dict(torch.load('best_bigru_model.pt'))\n","bigru_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = bigru_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","bigru_test_accuracy = correct / total\n","print(f'biGRU Test Accuracy: {bigru_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"4f1f661d-d038-44ce-a688-c19a6db7b74f","metadata":{"id":"4f1f661d-d038-44ce-a688-c19a6db7b74f"},"outputs":[],"source":["# CNN Model\n","class SentimentCNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentCNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128, 64)\n","        self.fc2 = nn.Linear(64, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x).permute(0, 2, 1)\n","        conv_out = self.conv1(embeds)\n","        pool_out = torch.max(conv_out, dim=-1)[0]\n","        out = self.dropout(pool_out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"id":"4559d670-cf2a-44e9-9c02-25d46f388241","metadata":{"id":"4559d670-cf2a-44e9-9c02-25d46f388241","outputId":"7072f1fa-278b-40f8-a599-828d8843427f"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Samuel Ng\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/30], Loss: 0.6628, Validation Accuracy: 0.7270\n","Epoch [2/30], Loss: 0.4245, Validation Accuracy: 0.7645\n","Epoch [3/30], Loss: 0.1890, Validation Accuracy: 0.7711\n","Epoch [4/30], Loss: 0.0689, Validation Accuracy: 0.7608\n","Epoch [5/30], Loss: 0.0259, Validation Accuracy: 0.7467\n","Epoch [6/30], Loss: 0.0147, Validation Accuracy: 0.7448\n","Epoch [7/30], Loss: 0.0059, Validation Accuracy: 0.7477\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Samuel Ng\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/30], Loss: 0.0047, Validation Accuracy: 0.7523\n","Early stopping!\n","CNN Test Accuracy: 0.7805\n"]}],"source":["# Instantiate the model\n","cnn_model = SentimentCNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(cnn_optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    cnn_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        cnn_optimizer.zero_grad()\n","        outputs = cnn_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1)  # Gradient clipping\n","        cnn_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    cnn_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = cnn_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(cnn_model.state_dict(), 'best_cnn_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set\n","cnn_model.load_state_dict(torch.load('best_cnn_model.pt'))\n","cnn_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = cnn_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","cnn_test_accuracy = correct / total\n","print(f'CNN Test Accuracy: {cnn_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"9f013a0d-b7d8-4c73-bd66-d6e9cdbaf080","metadata":{"id":"9f013a0d-b7d8-4c73-bd66-d6e9cdbaf080"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}