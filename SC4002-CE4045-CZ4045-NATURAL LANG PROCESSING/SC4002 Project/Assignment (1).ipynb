{"cells":[{"cell_type":"code","execution_count":2,"id":"c8c630ad","metadata":{"id":"c8c630ad","executionInfo":{"status":"ok","timestamp":1730515298712,"user_tz":-480,"elapsed":9,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"id":"e54b33d1","metadata":{"id":"e54b33d1","outputId":"0e2865c3-d567-439d-8dec-45f88c36b9a2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730515342868,"user_tz":-480,"elapsed":3433,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"markdown","id":"a3a78089","metadata":{"id":"a3a78089"},"source":["## Part 1"]},{"cell_type":"code","execution_count":5,"id":"98bbf012","metadata":{"id":"98bbf012","outputId":"55fb53db-2281-4823-ed93-41288e858314","colab":{"base_uri":"https://localhost:8080/","height":417,"referenced_widgets":["c2fb39b0ce094e3b842d6fe56de92352","bf5ff010943c4245abedb230baa57434","3b706dc519e54952b97bafe45d51e007","59f9d5cc551a4dddb649d24f8900d3c4","f7d5129cbbd74907b1fa53792fb1ab2c","bcf1f1a3c5e24cc6aa49dd20725b4290","23c800bc07d5428bbc46b63097c2d9a9","98c545d8d91a494cb38bbe63f25a2ebf","14bf1752a1e94c429ae043c172d99853","c205ea36d1a2469996880ec0b6710fb7","04b4b59605c54419a91edd5a91669a9b","760d2d962b674e3e99569ccc3c1f01e6","e4dcc976b6a44afc8e9c661095955fde","96f4108ba3624a2db180187c6184224a","80b9b99d650d4a66abdaa21a188c33a7","cc317488a7a94ec0bf6912ed3de12ba4","d14c9decb9614654a6b10de37d09f66f","828cabca7efa412aab1c939a0923d2a1","b3e90cd674c24f61888c243e907db91c","7d29dbb593fc4214ade38843a8b6646b","2729985b38464eaaa07c43fb8f9e4c00","09eb6b049ef14bc4ae88b1df044d4109","1eb2ce04d7b940c5b9a91cf53b9507b5","7fef3dd915734beeb67d76841f4a2a2f","180370b95a70412cb1604f94a7aaee4b","538f5426b90b4823878204684204d2b7","398a38f224054f9693c9ed5d7908432d","b6f766e995f645ed85d0a48a6c104926","e54102f069ba4de3ba0e31ef71e9c735","dd8297bc8f574c3cae51b884f2085e97","dc77a3b6e4a6443582933966ece1eef6","c5d4815d26814e5aaf831f086324a0db","9f025e1adb904a40b71f724790afd18e","551270aafb8e4c9f8bfa69717e29da8e","9c50e247ff97482db02ba261bba7f0a4","0fc30e1fd2284cb39e6e51acb5660510","a3b4b6c816a2466f85091985f4705ecc","08dadb2f7e0c4c63a0c00c204a7bac05","ef2eb6d773e6409db40a7620e150e61d","23b0a60da5624e99b8a2980a5d201819","a6405c439b2345528b3e4452ff2f2c99","3158884522b7480d94da8470f76ddb82","c10de4757a874c13bafd365d40d1cfee","bc0d4c746d0b4077970472846d4ffdf4","f403d5e4bb6b4da0b551e700c99e7a76","eda7af8e7f654eecb01fe381e36c4b7c","232956e7b89240c784d41bf16fe19db6","6d6d641ac25048a3b7f419ccc0d4a51a","7ab4fec875e340019afe5ecfa72d1a37","0b266f873c8f45e4a3e3d3eda70f1744","101d2f3e06c8421d99671c8cab921d31","505ab30d6c4d4c06beaffdb9fcd73026","71e55c9d7d614b2a8a5356bf69157b1e","ddeab69c61f24de3b55a4441b457e6fb","80b41c5523e14741864a0354d32e502e","0d9b279e8bad49f1acebcb7999cd10f3","14d3a0005e4d4e45b16cb9b99c91603c","0548ff9ecc814d7482b99a287e747262","0982904f156d477487d6b2a5b69f6e12","b770524daa4449d3a882660ca83a4f7b","4ee9ef37c34945e1aea9b0aa58866c0b","50fc4e5962ea4547bed3babbfb72eacf","aa5a1aedded84662b1d1aff16f7349b6","8563b9d6ac17413d8f88703ce76604af","bb1e506e32d447f4b745819b4d725ab7","b49fd9dd8f6445bd8322a04fb33f143e","9567b24426434c2b9c9a8a910b7b5c59","66f10ca934a04a9491c1250d3e651194","41b24308122f4546b5a7d9dc9fdd265a","9af9422b4bdd4a679ba8017e366ecb90","b48c7bf0781c4a9792d96865dca1283f","878af2317199415bb32abc164c8316c3","939e58a359ad4fc8b7ae34a827fd94a7","7fa213f4c0544054b5a31be2e472f59f","ebf25352d0404b0e9abf3129ab596b08","3a37d3e489264b51b28bb12befec5d5b","e62e1195dcf14271a295289eb5e9334c"]},"executionInfo":{"status":"ok","timestamp":1730515368526,"user_tz":-480,"elapsed":20093,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2fb39b0ce094e3b842d6fe56de92352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"760d2d962b674e3e99569ccc3c1f01e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eb2ce04d7b940c5b9a91cf53b9507b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551270aafb8e4c9f8bfa69717e29da8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f403d5e4bb6b4da0b551e700c99e7a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d9b279e8bad49f1acebcb7999cd10f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9567b24426434c2b9c9a8a910b7b5c59"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["The size of the vocabulary is: 18029\n"]}],"source":["# Question 1(a): What is the size of the vocabulary formed from your training data?\n","\n","from datasets import load_dataset\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","\n","# Initialize an empty set to store unique words\n","vocabulary = set()\n","\n","# Tokenize each review in the training dataset and update the vocabulary set\n","for text in train_dataset['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Print the size of the vocabulary\n","print(\"The size of the vocabulary is:\", len(vocabulary))"]},{"cell_type":"code","execution_count":null,"id":"b3f3bedd","metadata":{"id":"b3f3bedd","executionInfo":{"status":"aborted","timestamp":1730515298710,"user_tz":-480,"elapsed":10,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"id":"edb13d4a","metadata":{"id":"edb13d4a","outputId":"6a7385c7-83ed-493d-9e59-c7488bcf330e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730515413377,"user_tz":-480,"elapsed":335,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of OOV words in the training data: 16048\n"]}],"source":["# Question 1(b): How many OOV words exist in your training data?\n","\n","import numpy as np\n","\n","# Load the GloVe embeddings (make sure to download 'glove.6B.100d.txt' and place it in the working directory)\n","glove_vocab = set()\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        word = line.split()[0]\n","        glove_vocab.add(word)\n","\n","# Identify OOV words\n","oov_words = vocabulary - glove_vocab\n","\n","# Print the number of OOV words\n","print(\"Number of OOV words in the training data:\", len(oov_words))"]},{"cell_type":"code","execution_count":null,"id":"8b92c546","metadata":{"id":"8b92c546","executionInfo":{"status":"aborted","timestamp":1730515298711,"user_tz":-480,"elapsed":10,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":10,"id":"50688a71","metadata":{"id":"50688a71","outputId":"4ebfc7f1-edc9-4087-e456-0592d87b84ff","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730515422125,"user_tz":-480,"elapsed":312,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding matrix shape: (18029, 100)\n"]}],"source":["# Question 1(c): Mitigating OOV Words by Initializing Random Embeddings\n","\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","import numpy as np\n","\n","# Create mappings between words and indices\n","word2idx = {}\n","idx2word = {}\n","for idx, word in enumerate(vocabulary):\n","    word2idx[word] = idx\n","    idx2word[idx] = word\n","\n","# Initialize the embedding matrix with random values\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (len(vocabulary), embedding_dim))\n","\n","# Load GloVe embeddings into the embedding matrix where possible\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        glove_word = values[0]\n","        if glove_word in word2idx:\n","            vector = np.asarray(values[1:], dtype='float32')\n","            idx = word2idx[glove_word]\n","            embedding_matrix[idx] = vector\n","\n","# Now, embedding_matrix contains GloVe embeddings for known words and random values for OOV words\n","print(\"Embedding matrix shape:\", embedding_matrix.shape)"]},{"cell_type":"markdown","id":"c8eba559","metadata":{"id":"c8eba559"},"source":["## Part 2"]},{"cell_type":"code","execution_count":11,"id":"bff4efb2","metadata":{"id":"bff4efb2","outputId":"cfc07249-7113-4107-c960-8e559910cfa7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730518927592,"user_tz":-480,"elapsed":3499400,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6594, Validation Accuracy: 0.6482\n","Epoch [2/30], Loss: 0.5917, Validation Accuracy: 0.7242\n","Epoch [3/30], Loss: 0.5544, Validation Accuracy: 0.7073\n","Epoch [4/30], Loss: 0.5379, Validation Accuracy: 0.7233\n","Epoch [5/30], Loss: 0.5074, Validation Accuracy: 0.7223\n","Epoch [6/30], Loss: 0.4790, Validation Accuracy: 0.7542\n","Epoch [7/30], Loss: 0.4574, Validation Accuracy: 0.7552\n","Epoch [8/30], Loss: 0.4507, Validation Accuracy: 0.7467\n","Epoch [9/30], Loss: 0.4328, Validation Accuracy: 0.7655\n","Epoch [10/30], Loss: 0.4154, Validation Accuracy: 0.7617\n","Epoch [11/30], Loss: 0.4003, Validation Accuracy: 0.7617\n","Epoch [12/30], Loss: 0.3735, Validation Accuracy: 0.7570\n","Epoch [13/30], Loss: 0.3355, Validation Accuracy: 0.7674\n","Epoch [14/30], Loss: 0.3191, Validation Accuracy: 0.7777\n","Epoch [15/30], Loss: 0.2979, Validation Accuracy: 0.7552\n","Epoch [16/30], Loss: 0.2812, Validation Accuracy: 0.7674\n","Epoch [17/30], Loss: 0.2637, Validation Accuracy: 0.7730\n","Epoch [18/30], Loss: 0.2395, Validation Accuracy: 0.7730\n","Epoch [19/30], Loss: 0.2286, Validation Accuracy: 0.7617\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-11-3c2b1135c285>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7739\n"]}],"source":["# Import necessary libraries\n","from datasets import load_dataset\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","import torch.nn as nn\n","\n","# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = 'glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)\n","\n","# Define a function to convert sentences to indices\n","def sentence_to_indices(sentence, word2idx):\n","    tokens = word_tokenize(sentence.lower())\n","    indices = []\n","    for token in tokens:\n","        if token in word2idx:\n","            indices.append(word2idx[token])\n","        else:\n","            indices.append(word2idx['<unk>'])  # Map unknown words to '<unk>'\n","    return indices\n","\n","# Define the custom Dataset class\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, word2idx):\n","        self.texts = texts\n","        self.labels = labels\n","        self.word2idx = word2idx\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        indices = sentence_to_indices(text, self.word2idx)\n","        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n","\n","# Create datasets for training, validation, and testing\n","train_dataset = SentimentDataset(train_raw['text'], train_raw['label'], word2idx)\n","val_dataset = SentimentDataset(validation_raw['text'], validation_raw['label'], word2idx)\n","test_dataset = SentimentDataset(test_raw['text'], test_raw['label'], word2idx)\n","\n","# Define the collate_fn function for padding within batches\n","def collate_fn(batch):\n","    sequences = [item[0] for item in batch]\n","    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n","    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=word2idx['<pad>'])\n","    return sequences_padded, labels\n","\n","# Create DataLoaders for training, validation, and testing\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","\n","# Define the RNN model using pre-trained embeddings\n","class SentimentRNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n","# Instantiate the model\n","model = SentimentRNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","\n","\n","# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":13,"id":"d25d2ba3-a6df-4a02-905d-ca10c4b6f7c2","metadata":{"id":"d25d2ba3-a6df-4a02-905d-ca10c4b6f7c2","executionInfo":{"status":"ok","timestamp":1730518931148,"user_tz":-480,"elapsed":330,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["#3.1"]},{"cell_type":"code","execution_count":12,"id":"648ea703-d9ac-4256-8862-a5d6231fbbd6","metadata":{"id":"648ea703-d9ac-4256-8862-a5d6231fbbd6","executionInfo":{"status":"ok","timestamp":1730518928588,"user_tz":-480,"elapsed":2,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["class SentimentRNN_UpdateEmbeddings(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN_UpdateEmbeddings, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True  # Update the embeddings during training\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":14,"id":"7845f038-3527-47fb-aa91-18912dab8858","metadata":{"id":"7845f038-3527-47fb-aa91-18912dab8858","outputId":"0ac8b48c-a3f7-4760-b106-9a513782b124","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730520963128,"user_tz":-480,"elapsed":2029710,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6386, Validation Accuracy: 0.7261\n","Epoch [2/30], Loss: 0.4972, Validation Accuracy: 0.7523\n","Epoch [3/30], Loss: 0.3437, Validation Accuracy: 0.7674\n","Epoch [4/30], Loss: 0.2113, Validation Accuracy: 0.7617\n","Epoch [5/30], Loss: 0.1157, Validation Accuracy: 0.7636\n","Epoch [6/30], Loss: 0.0626, Validation Accuracy: 0.7692\n","Epoch [7/30], Loss: 0.0357, Validation Accuracy: 0.7477\n","Epoch [8/30], Loss: 0.0227, Validation Accuracy: 0.7523\n","Epoch [9/30], Loss: 0.0122, Validation Accuracy: 0.7523\n","Epoch [10/30], Loss: 0.0035, Validation Accuracy: 0.7495\n","Epoch [11/30], Loss: 0.0018, Validation Accuracy: 0.7533\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-41bf73be0b91>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model_update_embeddings.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7974\n"]}],"source":["# Instantiate the model\n","model = SentimentRNN_UpdateEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model_update_embeddings.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model_update_embeddings.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"4f4871eb-ca38-4b34-aeb5-71f8681c099f","metadata":{"id":"4f4871eb-ca38-4b34-aeb5-71f8681c099f","executionInfo":{"status":"aborted","timestamp":1730515298711,"user_tz":-480,"elapsed":9,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["#3.2"]},{"cell_type":"code","execution_count":15,"id":"f4a04cf4-0aa3-4466-bbb3-677bf4d75aed","metadata":{"id":"f4a04cf4-0aa3-4466-bbb3-677bf4d75aed","executionInfo":{"status":"ok","timestamp":1730521607274,"user_tz":-480,"elapsed":18357,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["# Load the GloVe embeddings\n","glove_vocab = set()\n","with open('glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        word = line.split()[0]\n","        glove_vocab.add(word)\n","\n","# Initialize the embedding matrix with random values\n","embedding_dim = 100\n","vocab_size = len(vocabulary)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Create a dictionary to store the GloVe embeddings\n","glove_embeddings = {}\n","glove_file = 'glove.6B.100d.txt'\n","with open(glove_file, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        glove_word = values[0]\n","        vector = np.asarray(values[1:], dtype='float32')\n","        # Normalize the loaded vectors to have unit norm\n","        vector /= np.linalg.norm(vector)\n","        glove_embeddings[glove_word] = vector\n","\n","# Load GloVe embeddings into the embedding matrix where possible\n","for idx, word in enumerate(vocabulary):\n","    if word in glove_embeddings:\n","        embedding_matrix[idx] = glove_embeddings[word]"]},{"cell_type":"code","execution_count":16,"id":"6a2a4400-44f8-40a6-93f3-683c5dc8df9a","metadata":{"id":"6a2a4400-44f8-40a6-93f3-683c5dc8df9a","executionInfo":{"status":"ok","timestamp":1730521613452,"user_tz":-480,"elapsed":331,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["class SentimentRNN_OOV(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN_OOV, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":17,"id":"893eaca0-ee17-4434-a4c3-014ddcc1e452","metadata":{"id":"893eaca0-ee17-4434-a4c3-014ddcc1e452","outputId":"f3314bb6-02b4-4088-87d1-fffbe463c383","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730522896242,"user_tz":-480,"elapsed":1280657,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6439, Validation Accuracy: 0.7458\n","Epoch [2/30], Loss: 0.4071, Validation Accuracy: 0.7533\n","Epoch [3/30], Loss: 0.2064, Validation Accuracy: 0.7411\n","Epoch [4/30], Loss: 0.1132, Validation Accuracy: 0.7514\n","Epoch [5/30], Loss: 0.0660, Validation Accuracy: 0.7477\n","Epoch [6/30], Loss: 0.0235, Validation Accuracy: 0.7439\n","Epoch [7/30], Loss: 0.0137, Validation Accuracy: 0.7458\n","Early stopping!\n"]}],"source":["\n","# Instantiate the model\n","model = SentimentRNN_OOV(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(model.state_dict(), 'best_model_oov.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n"]},{"cell_type":"code","execution_count":18,"id":"5cb26e55-c0b7-4341-accd-7693a04c12ed","metadata":{"id":"5cb26e55-c0b7-4341-accd-7693a04c12ed","outputId":"1e692c10-2a98-4891-ddcf-b357a6681a16","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730524106485,"user_tz":-480,"elapsed":7055,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-c4ada12bb9c3>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_model_oov.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7730\n"]}],"source":["# Load the best model and evaluate on the test set\n","model.load_state_dict(torch.load('best_model_oov.pt'))\n","model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","test_accuracy = correct / total\n","print(f'Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":19,"id":"ec35fbe0-eaf1-446e-ae13-38be9a9d9619","metadata":{"id":"ec35fbe0-eaf1-446e-ae13-38be9a9d9619","executionInfo":{"status":"ok","timestamp":1730524109021,"user_tz":-480,"elapsed":316,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["# biLSTM Model\n","class SentimentBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":20,"id":"66822818-6b88-4c9a-a24b-d3e2521c7976","metadata":{"id":"66822818-6b88-4c9a-a24b-d3e2521c7976","executionInfo":{"status":"ok","timestamp":1730524112723,"user_tz":-480,"elapsed":343,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["# biLSTM Model\n","class SentimentBiLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n","# biGRU Model\n","class SentimentBiGRU(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiGRU, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.gru = nn.GRU(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        gru_out, h_n = self.gru(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n","\n","\n"]},{"cell_type":"code","execution_count":21,"id":"8b85e41b-0b7c-4a92-a2e4-35b26a260753","metadata":{"id":"8b85e41b-0b7c-4a92-a2e4-35b26a260753","executionInfo":{"status":"ok","timestamp":1730524116661,"user_tz":-480,"elapsed":314,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["# Instantiate the models\n","bilstm_model = SentimentBiLSTM(embedding_matrix)\n","bigru_model = SentimentBiGRU(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","bilstm_model.to(device)\n","bigru_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","bilstm_optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=0.001, weight_decay=1e-5)\n","bigru_optimizer = torch.optim.Adam(bigru_model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(bilstm_optimizer, 'max', patience=2, factor=0.5, verbose=True)"]},{"cell_type":"code","execution_count":22,"id":"97491727-7b18-4358-8347-f5948c55e741","metadata":{"id":"97491727-7b18-4358-8347-f5948c55e741","outputId":"1bd93d5f-63bd-460a-a23e-082b8df04abb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730525957557,"user_tz":-480,"elapsed":1837935,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6693, Validation Accuracy: 0.7326\n","Epoch [2/30], Loss: 0.4406, Validation Accuracy: 0.7467\n","Epoch [3/30], Loss: 0.2294, Validation Accuracy: 0.7373\n","Epoch [4/30], Loss: 0.1194, Validation Accuracy: 0.7392\n","Epoch [5/30], Loss: 0.0693, Validation Accuracy: 0.7561\n","Epoch [6/30], Loss: 0.0394, Validation Accuracy: 0.7533\n","Epoch [7/30], Loss: 0.0186, Validation Accuracy: 0.7439\n","Epoch [8/30], Loss: 0.0210, Validation Accuracy: 0.7439\n","Epoch [9/30], Loss: 0.0077, Validation Accuracy: 0.7477\n","Epoch [10/30], Loss: 0.0017, Validation Accuracy: 0.7345\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-22-1d999e0fdf7e>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  bilstm_model.load_state_dict(torch.load('best_bilstm_model.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["biLSTM Test Accuracy: 0.7702\n"]}],"source":["# Training loop with validation and early stopping for biLSTM\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    bilstm_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        bilstm_optimizer.zero_grad()\n","        outputs = bilstm_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(bilstm_model.parameters(), max_norm=1)  # Gradient clipping\n","        bilstm_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    bilstm_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = bilstm_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(bilstm_model.state_dict(), 'best_bilstm_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set for biLSTM\n","bilstm_model.load_state_dict(torch.load('best_bilstm_model.pt'))\n","bilstm_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = bilstm_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","bilstm_test_accuracy = correct / total\n","print(f'biLSTM Test Accuracy: {bilstm_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":23,"id":"66d3c019-9c12-4dd9-9346-d66f1f9831a0","metadata":{"id":"66d3c019-9c12-4dd9-9346-d66f1f9831a0","outputId":"83feda3b-4042-46d9-8c20-1f76417ed107","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730528336780,"user_tz":-480,"elapsed":881555,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6342, Validation Accuracy: 0.7608\n","Epoch [2/30], Loss: 0.3909, Validation Accuracy: 0.7664\n","Epoch [3/30], Loss: 0.1859, Validation Accuracy: 0.7495\n","Epoch [4/30], Loss: 0.0921, Validation Accuracy: 0.7430\n","Epoch [5/30], Loss: 0.0497, Validation Accuracy: 0.7467\n","Epoch [6/30], Loss: 0.0251, Validation Accuracy: 0.7373\n","Epoch [7/30], Loss: 0.0172, Validation Accuracy: 0.7251\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-23-9a625a24d354>:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  bigru_model.load_state_dict(torch.load('best_bigru_model.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["biGRU Test Accuracy: 0.8011\n"]}],"source":["# Training loop with validation and early stopping for biGRU\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    bigru_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        bigru_optimizer.zero_grad()\n","        outputs = bigru_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(bigru_model.parameters(), max_norm=1)  # Gradient clipping\n","        bigru_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    bigru_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = bigru_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(bigru_model.state_dict(), 'best_bigru_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set for biGRU\n","bigru_model.load_state_dict(torch.load('best_bigru_model.pt'))\n","bigru_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = bigru_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","bigru_test_accuracy = correct / total\n","print(f'biGRU Test Accuracy: {bigru_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":24,"id":"4f1f661d-d038-44ce-a688-c19a6db7b74f","metadata":{"id":"4f1f661d-d038-44ce-a688-c19a6db7b74f","executionInfo":{"status":"ok","timestamp":1730529376130,"user_tz":-480,"elapsed":335,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":["# CNN Model\n","class SentimentCNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentCNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float32))\n","        self.embedding.weight.requires_grad = True\n","        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(128, 64)\n","        self.fc2 = nn.Linear(64, 1)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x).permute(0, 2, 1)\n","        conv_out = self.conv1(embeds)\n","        pool_out = torch.max(conv_out, dim=-1)[0]\n","        out = self.dropout(pool_out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":25,"id":"4559d670-cf2a-44e9-9c02-25d46f388241","metadata":{"id":"4559d670-cf2a-44e9-9c02-25d46f388241","outputId":"c7d25e69-7716-43ae-e838-355d58cd5340","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730529448923,"user_tz":-480,"elapsed":69860,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Loss: 0.6602, Validation Accuracy: 0.7129\n","Epoch [2/30], Loss: 0.4174, Validation Accuracy: 0.7702\n","Epoch [3/30], Loss: 0.1891, Validation Accuracy: 0.7692\n","Epoch [4/30], Loss: 0.0739, Validation Accuracy: 0.7589\n","Epoch [5/30], Loss: 0.0294, Validation Accuracy: 0.7617\n","Epoch [6/30], Loss: 0.0131, Validation Accuracy: 0.7598\n","Epoch [7/30], Loss: 0.0063, Validation Accuracy: 0.7645\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-25-b31d088b4292>:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  cnn_model.load_state_dict(torch.load('best_cnn_model.pt'))\n"]},{"output_type":"stream","name":"stdout","text":["CNN Test Accuracy: 0.7814\n"]}],"source":["# Instantiate the model\n","cnn_model = SentimentCNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(cnn_optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","best_val_accuracy = 0\n","epochs_no_improve = 0\n","\n","for epoch in range(num_epochs):\n","    cnn_model.train()\n","    running_loss = 0.0\n","    for sequences, labels in train_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        cnn_optimizer.zero_grad()\n","        outputs = cnn_model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(cnn_model.parameters(), max_norm=1)  # Gradient clipping\n","        cnn_optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","\n","    # Validation\n","    cnn_model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in val_loader:\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = cnn_model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","\n","    # Learning rate scheduling\n","    scheduler.step(val_accuracy)\n","\n","    # Check for improvement\n","    if val_accuracy > best_val_accuracy:\n","        best_val_accuracy = val_accuracy\n","        epochs_no_improve = 0\n","        # Save the best model\n","        torch.save(cnn_model.state_dict(), 'best_cnn_model.pt')\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print('Early stopping!')\n","            break\n","\n","# Load the best model and evaluate on the test set\n","cnn_model.load_state_dict(torch.load('best_cnn_model.pt'))\n","cnn_model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for sequences, labels in test_loader:\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        outputs = cnn_model(sequences)\n","        predicted = (outputs >= 0.5).long()\n","        correct += (predicted == labels.long()).sum().item()\n","        total += labels.size(0)\n","cnn_test_accuracy = correct / total\n","print(f'CNN Test Accuracy: {cnn_test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":null,"id":"9f013a0d-b7d8-4c73-bd66-d6e9cdbaf080","metadata":{"id":"9f013a0d-b7d8-4c73-bd66-d6e9cdbaf080","executionInfo":{"status":"aborted","timestamp":1730515298712,"user_tz":-480,"elapsed":9,"user":{"displayName":"Shan Shan","userId":"00333815406818419816"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"c2fb39b0ce094e3b842d6fe56de92352":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf5ff010943c4245abedb230baa57434","IPY_MODEL_3b706dc519e54952b97bafe45d51e007","IPY_MODEL_59f9d5cc551a4dddb649d24f8900d3c4"],"layout":"IPY_MODEL_f7d5129cbbd74907b1fa53792fb1ab2c"}},"bf5ff010943c4245abedb230baa57434":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcf1f1a3c5e24cc6aa49dd20725b4290","placeholder":"​","style":"IPY_MODEL_23c800bc07d5428bbc46b63097c2d9a9","value":"README.md: 100%"}},"3b706dc519e54952b97bafe45d51e007":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98c545d8d91a494cb38bbe63f25a2ebf","max":7457,"min":0,"orientation":"horizontal","style":"IPY_MODEL_14bf1752a1e94c429ae043c172d99853","value":7457}},"59f9d5cc551a4dddb649d24f8900d3c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c205ea36d1a2469996880ec0b6710fb7","placeholder":"​","style":"IPY_MODEL_04b4b59605c54419a91edd5a91669a9b","value":" 7.46k/7.46k [00:00&lt;00:00, 349kB/s]"}},"f7d5129cbbd74907b1fa53792fb1ab2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcf1f1a3c5e24cc6aa49dd20725b4290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23c800bc07d5428bbc46b63097c2d9a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98c545d8d91a494cb38bbe63f25a2ebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14bf1752a1e94c429ae043c172d99853":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c205ea36d1a2469996880ec0b6710fb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04b4b59605c54419a91edd5a91669a9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"760d2d962b674e3e99569ccc3c1f01e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4dcc976b6a44afc8e9c661095955fde","IPY_MODEL_96f4108ba3624a2db180187c6184224a","IPY_MODEL_80b9b99d650d4a66abdaa21a188c33a7"],"layout":"IPY_MODEL_cc317488a7a94ec0bf6912ed3de12ba4"}},"e4dcc976b6a44afc8e9c661095955fde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d14c9decb9614654a6b10de37d09f66f","placeholder":"​","style":"IPY_MODEL_828cabca7efa412aab1c939a0923d2a1","value":"train.parquet: 100%"}},"96f4108ba3624a2db180187c6184224a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3e90cd674c24f61888c243e907db91c","max":698845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d29dbb593fc4214ade38843a8b6646b","value":698845}},"80b9b99d650d4a66abdaa21a188c33a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2729985b38464eaaa07c43fb8f9e4c00","placeholder":"​","style":"IPY_MODEL_09eb6b049ef14bc4ae88b1df044d4109","value":" 699k/699k [00:00&lt;00:00, 3.80MB/s]"}},"cc317488a7a94ec0bf6912ed3de12ba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d14c9decb9614654a6b10de37d09f66f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828cabca7efa412aab1c939a0923d2a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3e90cd674c24f61888c243e907db91c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d29dbb593fc4214ade38843a8b6646b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2729985b38464eaaa07c43fb8f9e4c00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09eb6b049ef14bc4ae88b1df044d4109":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eb2ce04d7b940c5b9a91cf53b9507b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fef3dd915734beeb67d76841f4a2a2f","IPY_MODEL_180370b95a70412cb1604f94a7aaee4b","IPY_MODEL_538f5426b90b4823878204684204d2b7"],"layout":"IPY_MODEL_398a38f224054f9693c9ed5d7908432d"}},"7fef3dd915734beeb67d76841f4a2a2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6f766e995f645ed85d0a48a6c104926","placeholder":"​","style":"IPY_MODEL_e54102f069ba4de3ba0e31ef71e9c735","value":"validation.parquet: 100%"}},"180370b95a70412cb1604f94a7aaee4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd8297bc8f574c3cae51b884f2085e97","max":90001,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc77a3b6e4a6443582933966ece1eef6","value":90001}},"538f5426b90b4823878204684204d2b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5d4815d26814e5aaf831f086324a0db","placeholder":"​","style":"IPY_MODEL_9f025e1adb904a40b71f724790afd18e","value":" 90.0k/90.0k [00:00&lt;00:00, 4.41MB/s]"}},"398a38f224054f9693c9ed5d7908432d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6f766e995f645ed85d0a48a6c104926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54102f069ba4de3ba0e31ef71e9c735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd8297bc8f574c3cae51b884f2085e97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc77a3b6e4a6443582933966ece1eef6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c5d4815d26814e5aaf831f086324a0db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f025e1adb904a40b71f724790afd18e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"551270aafb8e4c9f8bfa69717e29da8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c50e247ff97482db02ba261bba7f0a4","IPY_MODEL_0fc30e1fd2284cb39e6e51acb5660510","IPY_MODEL_a3b4b6c816a2466f85091985f4705ecc"],"layout":"IPY_MODEL_08dadb2f7e0c4c63a0c00c204a7bac05"}},"9c50e247ff97482db02ba261bba7f0a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef2eb6d773e6409db40a7620e150e61d","placeholder":"​","style":"IPY_MODEL_23b0a60da5624e99b8a2980a5d201819","value":"test.parquet: 100%"}},"0fc30e1fd2284cb39e6e51acb5660510":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6405c439b2345528b3e4452ff2f2c99","max":92206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3158884522b7480d94da8470f76ddb82","value":92206}},"a3b4b6c816a2466f85091985f4705ecc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c10de4757a874c13bafd365d40d1cfee","placeholder":"​","style":"IPY_MODEL_bc0d4c746d0b4077970472846d4ffdf4","value":" 92.2k/92.2k [00:00&lt;00:00, 5.68MB/s]"}},"08dadb2f7e0c4c63a0c00c204a7bac05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2eb6d773e6409db40a7620e150e61d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b0a60da5624e99b8a2980a5d201819":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6405c439b2345528b3e4452ff2f2c99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3158884522b7480d94da8470f76ddb82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c10de4757a874c13bafd365d40d1cfee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc0d4c746d0b4077970472846d4ffdf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f403d5e4bb6b4da0b551e700c99e7a76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eda7af8e7f654eecb01fe381e36c4b7c","IPY_MODEL_232956e7b89240c784d41bf16fe19db6","IPY_MODEL_6d6d641ac25048a3b7f419ccc0d4a51a"],"layout":"IPY_MODEL_7ab4fec875e340019afe5ecfa72d1a37"}},"eda7af8e7f654eecb01fe381e36c4b7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b266f873c8f45e4a3e3d3eda70f1744","placeholder":"​","style":"IPY_MODEL_101d2f3e06c8421d99671c8cab921d31","value":"Generating train split: 100%"}},"232956e7b89240c784d41bf16fe19db6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_505ab30d6c4d4c06beaffdb9fcd73026","max":8530,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71e55c9d7d614b2a8a5356bf69157b1e","value":8530}},"6d6d641ac25048a3b7f419ccc0d4a51a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddeab69c61f24de3b55a4441b457e6fb","placeholder":"​","style":"IPY_MODEL_80b41c5523e14741864a0354d32e502e","value":" 8530/8530 [00:00&lt;00:00, 9129.64 examples/s]"}},"7ab4fec875e340019afe5ecfa72d1a37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b266f873c8f45e4a3e3d3eda70f1744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"101d2f3e06c8421d99671c8cab921d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"505ab30d6c4d4c06beaffdb9fcd73026":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71e55c9d7d614b2a8a5356bf69157b1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddeab69c61f24de3b55a4441b457e6fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80b41c5523e14741864a0354d32e502e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d9b279e8bad49f1acebcb7999cd10f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_14d3a0005e4d4e45b16cb9b99c91603c","IPY_MODEL_0548ff9ecc814d7482b99a287e747262","IPY_MODEL_0982904f156d477487d6b2a5b69f6e12"],"layout":"IPY_MODEL_b770524daa4449d3a882660ca83a4f7b"}},"14d3a0005e4d4e45b16cb9b99c91603c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ee9ef37c34945e1aea9b0aa58866c0b","placeholder":"​","style":"IPY_MODEL_50fc4e5962ea4547bed3babbfb72eacf","value":"Generating validation split: 100%"}},"0548ff9ecc814d7482b99a287e747262":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa5a1aedded84662b1d1aff16f7349b6","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8563b9d6ac17413d8f88703ce76604af","value":1066}},"0982904f156d477487d6b2a5b69f6e12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb1e506e32d447f4b745819b4d725ab7","placeholder":"​","style":"IPY_MODEL_b49fd9dd8f6445bd8322a04fb33f143e","value":" 1066/1066 [00:00&lt;00:00, 30289.12 examples/s]"}},"b770524daa4449d3a882660ca83a4f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ee9ef37c34945e1aea9b0aa58866c0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50fc4e5962ea4547bed3babbfb72eacf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa5a1aedded84662b1d1aff16f7349b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8563b9d6ac17413d8f88703ce76604af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb1e506e32d447f4b745819b4d725ab7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b49fd9dd8f6445bd8322a04fb33f143e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9567b24426434c2b9c9a8a910b7b5c59":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66f10ca934a04a9491c1250d3e651194","IPY_MODEL_41b24308122f4546b5a7d9dc9fdd265a","IPY_MODEL_9af9422b4bdd4a679ba8017e366ecb90"],"layout":"IPY_MODEL_b48c7bf0781c4a9792d96865dca1283f"}},"66f10ca934a04a9491c1250d3e651194":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_878af2317199415bb32abc164c8316c3","placeholder":"​","style":"IPY_MODEL_939e58a359ad4fc8b7ae34a827fd94a7","value":"Generating test split: 100%"}},"41b24308122f4546b5a7d9dc9fdd265a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fa213f4c0544054b5a31be2e472f59f","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebf25352d0404b0e9abf3129ab596b08","value":1066}},"9af9422b4bdd4a679ba8017e366ecb90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a37d3e489264b51b28bb12befec5d5b","placeholder":"​","style":"IPY_MODEL_e62e1195dcf14271a295289eb5e9334c","value":" 1066/1066 [00:00&lt;00:00, 35735.29 examples/s]"}},"b48c7bf0781c4a9792d96865dca1283f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"878af2317199415bb32abc164c8316c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939e58a359ad4fc8b7ae34a827fd94a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fa213f4c0544054b5a31be2e472f59f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebf25352d0404b0e9abf3129ab596b08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a37d3e489264b51b28bb12befec5d5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e62e1195dcf14271a295289eb5e9334c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}