{"cells":[{"cell_type":"code","source":["!pip install datasets afinn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4nhDXd-3J7L","executionInfo":{"status":"ok","timestamp":1730953444824,"user_tz":-480,"elapsed":10993,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"f143ac47-767d-4efc-eb4a-f3e876a22ddd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n","Collecting afinn\n","  Downloading afinn-0.1.tar.gz (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n","Building wheels for collected packages: afinn\n","  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=5f504abac4eb8071ca534ba5d976bcc70ea106e645d753855b09fb6d5d288c62\n","  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n","Successfully built afinn\n","Installing collected packages: afinn\n","Successfully installed afinn-0.1\n"]}]},{"cell_type":"code","source":["import nltk\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","\n","nltk.download('punkt_tab')\n","from datasets import load_dataset\n","from tokenizers import ByteLevelBPETokenizer\n","from tqdm import tqdm\n","\n","nltk.download('punkt')\n","import os\n","import random\n","from collections import defaultdict\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from afinn import Afinn\n","from sklearn.metrics import accuracy_score, f1_score\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import (AdamW, BertForSequenceClassification, BertTokenizer,\n","                          get_linear_schedule_with_warmup)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m11psr0k2_H_","executionInfo":{"status":"ok","timestamp":1730953457763,"user_tz":-480,"elapsed":8844,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"b3d1f752-390b-49b7-8525-71bc8a1c251b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"sSuFuE3f2y11"},"source":["# Part 0. Dataset Preparation"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365,"referenced_widgets":["b9a20ae5237548e6a80abb860bac6323","b80014742c5e42dab47b98a2b9b94fe5","142a8e61f9b348fc95608e2bc16cc66c","0e5d36ca14d540ceba753ede89b9a06a","c87930e8a376406c84b5fb5f07c4862e","e182b43be96042bb8bf5b58171331266","d2dd6e28f4414dc98880da0eaa36d60b","db29269049944175afddb83bee682d62","65f9112e474f49f48272d2d3cd9c4ab4","4328e6389a8243cb9e1d5fe4b5559124","1758b9a77ef04110b7dec1aa0d0b4907","6913f74db4944ead821c2b7d82407a2a","2fbc2ee839a24c049c8090ba65968ba8","56af98a93bf14140a7904ba946d359ac","45833cc283d3452a80b36fd4bcbcaa24","d2a54c522b574b54a3b39428d48652d2","044ada3fb50f49f2b494ce515b39f7ef","081f2af61989433d8ada5474f1a36519","d0ddd09733fc4863befb570409a72a73","5d4a16f91e5a4824bc9e4bea806e9280","053f958c93b1456697b339829664ade8","f1f23bce20bf4f029733749e63b8822e","65b8b8cd83ca48b68fe9a6a76bab560a","a5dbc27ea6224e03a4eb58874f843767","ed9c200a935a48cbb402a1847591b0a4","a483d2b1b36545fa8ebbb95bd346a6df","323c0159c41947c681a2c41cc95f8fd0","db1370de113f4a2881f6807d1b543ce0","a8f77847effd4dfea7615a85f202f094","1a8fb22104db4678926a639c6ea5f150","57edbfb3cda44e59bae8d10dfbc2fced","0587627f55a84e039157191f35bb848b","e98aeb97e8934272964f352e40193b75","067ee71d667d4f9e81c2abc086dae57c","e6845c31839a48109903d3e492f65be0","b23b67ea7970468fa1ea1a3f238a3fb3","e72110ba54774890ae0fc79646a4bcb7","e62649bb5706440abd81204a1fbc386a","2d7dbe1140744abdbe758ace4a7dbe4d","301e4bf39051476fb81f4655ce2b0f50","65181e38ac0140a1b34d9cc90e9b008d","d5c26fb416e5432583a9d1f8ce56f030","6595fbebf3f643a7ab704e13bd472b3c","d9183cbed6624a4ab1fbfcd643684139","195cc754a96b4bdda1cad4ff56e2d5a3","92345bc213cb48d3b3ec7749c1d0f7f6","77ddc22da66d4eccb610938a2a2e0322","13372279ded9411e98acb56fede07b48","f23db805c4dc46cbbdffc1876e8003de","71f0228e70234679ae791d67136c4935","08edc4a257c54be68897d5348c1b7b73","e250d3ba2db84d908794c2258ddf0597","f7a76d5bf7c14ca7afbd88ff20c72d4f","3540ae7204ad41debee359a772d5249a","b036265172a84241bc4efc665abc3960","d874d461224742dab38b09532a57a76e","6b698d676ad946a78b01e0a4f2923545","b2d44d3bb1da464a87c1ad9e50ce509b","5279cc42c7a84a2fb00059f6018d1425","f908e6ceea0b4ccd8e03ec750bddf7fc","efe29be66846448295b1518447c0058b","a2924aef5f1b433a8252a4855c018c6d","2e13c1f2957d4b70ad4d9d32b9903167","80c97b139ff84b36bb0139c45151046c","eb94b9b807374752905d47796718ddfa","a65147124eb345eb8ba927a738d67f5d","801946a64d0442648420a6b8415b32f3","547c8db506994fda96f48fa935ed2b68","83a9cad4aff4448699735eb133b298ab","fa3c66466ddc4b35925d853fb13be101","4f128c6d43b64575978c0249b9c03807","0dae08162aa54c049312ced2903a178a","711728eb780946d38d5196c2493d9c9e","ee6e94bb8d56497cb63435dbccff0aa7","f9acf7fd111f4488a88ca5979f2ac4f2","9f7de7f3a86442d2ab0dc617585b34a7","399556dfd68f4f6b86426a5db89166b7"]},"id":"7Np-uLQU2y13","executionInfo":{"status":"ok","timestamp":1730953488394,"user_tz":-480,"elapsed":3482,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"5c6a41ea-0e9e-4822-eb3b-e2137bdf94bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.46k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a20ae5237548e6a80abb860bac6323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train.parquet:   0%|          | 0.00/699k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6913f74db4944ead821c2b7d82407a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["validation.parquet:   0%|          | 0.00/90.0k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b8b8cd83ca48b68fe9a6a76bab560a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test.parquet:   0%|          | 0.00/92.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067ee71d667d4f9e81c2abc086dae57c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195cc754a96b4bdda1cad4ff56e2d5a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d874d461224742dab38b09532a57a76e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801946a64d0442648420a6b8415b32f3"}},"metadata":{}}],"source":["dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","validation_dataset = dataset['validation']\n","test_dataset = dataset['test']"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXVpjQIv2y14","executionInfo":{"status":"ok","timestamp":1730953488394,"user_tz":-480,"elapsed":6,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"30fdbe02-cc36-4a95-a671-12750741c688"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'text': \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\", 'label': 1}\n"]}],"source":["print(train_dataset[4])"]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"352nuy9R4f_5","executionInfo":{"status":"ok","timestamp":1730953743974,"user_tz":-480,"elapsed":335,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"38e9beb1-1507-4a43-840c-5a09d739685d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"markdown","metadata":{"id":"yY91mSEv2y14"},"source":["# Part 1. Preparing Word Embeddings"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"T0B3eizh2y14","executionInfo":{"status":"ok","timestamp":1730953812884,"user_tz":-480,"elapsed":308,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def load_glove_embeddings(filepath):\n","    embeddings_index = {}\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    return embeddings_index"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"JHQYuQZS2y15","executionInfo":{"status":"ok","timestamp":1730953814047,"user_tz":-480,"elapsed":2,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def build_vocab(dataset):\n","    vocab = set()\n","    for sentence in dataset:\n","        tokens = word_tokenize(sentence['text'].lower())\n","        vocab.update(tokens)\n","    return vocab"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"I8Rs6Ytj2y16","executionInfo":{"status":"ok","timestamp":1730953815621,"user_tz":-480,"elapsed":324,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def find_oov_words(vocab, embeddings):\n","    oovWords = [ word for word in vocab if word not in embeddings]\n","    return oovWords"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"LWOLd0Ir2y17","executionInfo":{"status":"ok","timestamp":1730954005838,"user_tz":-480,"elapsed":19180,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["glove_embeddings = load_glove_embeddings('/glove.6B.100d.txt')"]},{"cell_type":"markdown","metadata":{"id":"eUbWTVCL2y17"},"source":["### Part 1a) What is the size of the vocabulary formed from your training data?"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kAVllZn82y17","executionInfo":{"status":"ok","timestamp":1730954015234,"user_tz":-480,"elapsed":3254,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"765e6169-d489-469d-cc09-02f98c603b3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of vocabulary in training data: 18029\n"]}],"source":["vocab = build_vocab(train_dataset)\n","print(f'Size of vocabulary in training data: {len(vocab)}')"]},{"cell_type":"markdown","metadata":{"id":"DgRah-wL2y18"},"source":["### Part 1b) How many OOV words exist in your training data?"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNMpq8Vz2y18","executionInfo":{"status":"ok","timestamp":1730954015234,"user_tz":-480,"elapsed":4,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"d7df1bc8-8cf2-49bd-e999-beacd292275b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of OOV words in training data: 2037\n"]}],"source":["oovWords = find_oov_words(vocab=vocab, embeddings=glove_embeddings)\n","print(f'Number of OOV words in training data: {len(oovWords)}')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"P15xNuQk2y19","executionInfo":{"status":"ok","timestamp":1730954015234,"user_tz":-480,"elapsed":3,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"U2ond9ty2y19"},"source":["### Part 1c) OOV Strategies"]},{"cell_type":"markdown","metadata":{"id":"hov23L0_2y19"},"source":["#### Part 1ci) OOV Strategy 1: Random Embeddings"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"5get66Mv2y19","executionInfo":{"status":"ok","timestamp":1730954046359,"user_tz":-480,"elapsed":20393,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"markdown","metadata":{"id":"i9ZGeP792y19"},"source":["#### Part 1cii) OOV Strategy 2: Byte-pair Encoding"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMGWTc3N2y1-","executionInfo":{"status":"ok","timestamp":1730954049620,"user_tz":-480,"elapsed":3263,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"735b7ea9-aa09-493f-8765-6fccb1021b68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized sample from training set: ['the', 'Ġrock', 'Ġis', 'Ġdestined', 'Ġto', 'Ġbe', 'Ġthe', 'Ġ21', 'st', 'Ġcentury', \"'s\", 'Ġnew', 'Ġ\"', 'Ġconan', 'Ġ\"', 'Ġand', 'Ġthat', 'Ġhe', \"'s\", 'Ġgoing', 'Ġto', 'Ġmake', 'Ġa', 'Ġsplash', 'Ġeven', 'Ġgreater', 'Ġthan', 'Ġarnold', 'Ġschwarzenegger', 'Ġ,', 'Ġjean', '-', 'cl', 'aud', 'Ġvan', 'Ġdamme', 'Ġor', 'Ġsteven', 'Ġse', 'gal', 'Ġ.']\n"]}],"source":["# Load the Rotten Tomatoes dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Prepare the text data for BPE training\n","train_texts = [entry['text'] for entry in train_raw]\n","\n","# Step 3: Initialize and Train the BPE Tokenizer\n","# Define a ByteLevelBPETokenizer instance\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# Train the tokenizer on the training texts\n","tokenizer.train_from_iterator(train_texts, vocab_size=30000, min_frequency=2)\n","\n","# Save the trained tokenizer (optional)\n","tokenizer.save_model(\".\", \"bpe_tokenizer\")\n","\n","# Step 4: Apply the Trained BPE Tokenizer to Your Datasets\n","# Tokenize a sample from the training set\n","train_tokenized = [tokenizer.encode(text).tokens for text in train_texts]\n","\n","# Similarly tokenize the validation and test sets\n","validation_texts = [entry['text'] for entry in validation_raw]\n","validation_tokenized = [tokenizer.encode(text).tokens for text in validation_texts]\n","\n","test_texts = [entry['text'] for entry in test_raw]\n","test_tokenized = [tokenizer.encode(text).tokens for text in test_texts]\n","\n","# Print a sample tokenized text\n","print(\"Tokenized sample from training set:\", train_tokenized[0])"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Qzvg-lre2y1-","executionInfo":{"status":"ok","timestamp":1730954049621,"user_tz":-480,"elapsed":4,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def load_glove_embeddings(filepath):\n","    embeddings_index = {}\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","def get_subword_embedding(subwords, glove_embeddings, embedding_dim=100):\n","    vectors = []\n","    for subword in subwords:\n","        if subword in glove_embeddings:\n","            vectors.append(glove_embeddings[subword])\n","    if not vectors:\n","        # Handle the case where no subwords have embeddings\n","        return np.random.uniform(-0.05, 0.05, (embedding_dim,))\n","    # Average or sum the vectors\n","    return np.mean(vectors, axis=0)  # For averaging\n","    # return np.sum(vectors, axis=0)   # For summation"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"cuwnzhn12y1-","executionInfo":{"status":"ok","timestamp":1730954089158,"user_tz":-480,"elapsed":21220,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","glove_embeddings = load_glove_embeddings('/glove.6B.100d.txt')\n","tokenizer = ByteLevelBPETokenizer.from_file(\"bpe_tokenizer-vocab.json\", \"bpe_tokenizer-merges.txt\")\n","\n","for word in word2idx:\n","    idx = word2idx[word]\n","    if word in glove_embeddings:\n","        vector = glove_embeddings[word]\n","        embedding_matrix[idx] = vector\n","\n","    else:\n","        subwords = tokenizer.encode(word).tokens\n","        # print(subwords)\n","        vector = get_subword_embedding(subwords=subwords, glove_embeddings=glove_embeddings)\n","        embedding_matrix[idx] = vector\n","\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"markdown","metadata":{"id":"hmpjmbpN2y1-"},"source":["#### Part 1ci) OOV Strategy 3: Backoff and Interpolation"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXxc-CW92y1-","executionInfo":{"status":"ok","timestamp":1730954153774,"user_tz":-480,"elapsed":32275,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"e667961a-37bb-4e19-de7d-1dc5721d594c"},"outputs":[{"output_type":"stream","name":"stdout","text":["The size of the vocabulary is: 18029\n","Embedding matrix shape: (18031, 100)\n"]}],"source":["\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","vocabulary = set()\n","\n","for text in train_dataset['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Print the size of the vocabulary\n","print(\"The size of the vocabulary is:\", len(vocabulary))\n","\n","glove_vocab = set()\n","glove_embeddings = {}\n","embedding_dim = 100\n","with open('/glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        glove_vocab.add(word)\n","        glove_embeddings[word] = np.asarray(values[1:], dtype='float32')\n","\n","# Identify OOV words\n","oov_words = vocabulary - glove_vocab\n","\n","# Question 1(c): Handle OOV words using ngram and backoff and interpolation\n","\n","from collections import defaultdict\n","\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n","\n","# Initialize the embedding matrix for known words\n","embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n","for word in glove_vocab:\n","    if word in word2idx:  # Check if the word is in word2idx\n","        idx = word2idx[word]\n","        embedding_matrix[idx] = glove_embeddings[word]\n","\n","\n","# Create n-grams from training data\n","def create_ngrams(texts, n):\n","    ngram_counts = defaultdict(int)\n","    for entry in texts:  # Adjusted to iterate over each entry (dictionary)\n","        text = entry['text']  # Access the text with the appropriate key\n","        tokens = word_tokenize(text.lower())\n","        ngrams = zip(*[tokens[i:] for i in range(n)])  # Create n-grams\n","        for ngram in ngrams:\n","            ngram_counts[' '.join(ngram)] += 1\n","    return ngram_counts\n","\n","# Create bigrams and trigrams\n","bigram_counts = create_ngrams(train_dataset, 2)  # Use train_raw directly\n","trigram_counts = create_ngrams(train_dataset, 3)  # Use train_raw directly\n","\n","# Backoff and interpolation parameters\n","lambda1 = 0.8  # weight for unigrams\n","lambda2 = 0.15  # weight for bigrams\n","lambda3 = 0.05  # weight for trigrams\n","\n","# Define a function for backoff and interpolation handling OOV words\n","def get_ngram_embedding(word, embedding_matrix, word2idx, bigram_counts, trigram_counts):\n","    if word in word2idx:\n","        return embedding_matrix[word2idx[word]]\n","    else:\n","        # Initialize embeddings for interpolation\n","        unigram_embedding = np.zeros(embedding_matrix.shape[1])\n","        bigram_embedding = np.zeros(embedding_matrix.shape[1])\n","        trigram_embedding = np.zeros(embedding_matrix.shape[1])\n","\n","        # Check trigram context\n","        trigram_context = [ngram for ngram in trigram_counts if word in ngram.split()]\n","        if trigram_context:\n","            context_ngram = trigram_context[0].split()\n","            trigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in context_ngram if w in word2idx], axis=0)\n","\n","        # Check bigram context\n","        bigram_context = [ngram for ngram in bigram_counts if word in ngram.split()]\n","        if bigram_context:\n","            context_ngram = bigram_context[0].split()\n","            bigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in context_ngram if w in word2idx], axis=0)\n","\n","        # Calculate unigram embedding (mean of known embeddings, if available)\n","        known_words = [w for w in word2idx if w in vocabulary]\n","        if known_words:\n","            unigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in known_words if word2idx[w] < len(embedding_matrix)], axis=0)\n","\n","        # Apply interpolation\n","        final_embedding = (lambda1 * unigram_embedding) + (lambda2 * bigram_embedding) + (lambda3 * trigram_embedding)\n","        return final_embedding\n","\n","# Handle OOV words using n-gram embeddings\n","for oov_word in oov_words:\n","    if oov_word in word2idx:  # Ensure the OOV word exists in word2idx\n","        idx = word2idx[oov_word]\n","        embedding_matrix[idx] = get_ngram_embedding(oov_word, embedding_matrix, word2idx, bigram_counts, trigram_counts)\n","\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)\n","\n","# Now, the embedding_matrix contains GloVe embeddings for known words and n-gram based embeddings for OOV words\n","print(\"Embedding matrix shape:\", embedding_matrix.shape)"]},{"cell_type":"markdown","metadata":{"id":"XCvvSh_J2y1_"},"source":["# Part 2 RNN"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"X1pciIHN2y1_","executionInfo":{"status":"ok","timestamp":1730954188815,"user_tz":-480,"elapsed":525,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Define the RNN model using pre-trained embeddings\n","class SentimentRNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze= True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        out, _ = torch.max(lstm_out, dim=1)\n","        # Concatenate the final hidden states from both directions\n","        # out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"tR384anZ2y2A","executionInfo":{"status":"ok","timestamp":1730954203164,"user_tz":-480,"elapsed":14023,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","# embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"RuU4M3kS2y2A","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":11,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Define a function to convert sentences to indices\n","def sentence_to_indices(sentence, word2idx):\n","    tokens = word_tokenize(sentence.lower())\n","    indices = []\n","    for token in tokens:\n","        if token in word2idx:\n","            indices.append(word2idx[token])\n","        else:\n","            indices.append(word2idx['<unk>'])  # Map unknown words to '<unk>'\n","    return indices\n","\n","# Define the collate_fn function for padding within batches\n","def collate_fn(batch):\n","    sequences = [item[0] for item in batch]\n","    labels = torch.tensor([item[1] for item in batch], dtype=torch.float)\n","    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=word2idx['<pad>'])\n","    return sequences_padded, labels"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"fMFD3nwp2y2B","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":10,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Define the custom Dataset class\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, word2idx):\n","        self.texts = texts\n","        self.labels = labels\n","        self.word2idx = word2idx\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        indices = sentence_to_indices(text, self.word2idx)\n","        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"LW8ucGQ02y2B","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":10,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def train_loop(model, device, optimizer, criterion, train_loader):\n","    model.train()\n","    running_loss = 0.0\n","\n","    for sequences, labels in tqdm(train_loader):\n","        sequences = sequences.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(sequences)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)  # Gradient clipping\n","        optimizer.step()\n","        running_loss += loss.item() * sequences.size(0)\n","    epoch_loss = running_loss / len(train_dataset)\n","    return epoch_loss"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Wp3BursR2y2B","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":10,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def evaluate_model(model, val_loader, criterion, device):\n","    # Validation\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    val_loss = 0\n","    with torch.no_grad():\n","        for sequences, labels in tqdm(val_loader):\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","\n","            #calculate val loss\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    val_accuracy = correct / total\n","    val_loss /= len(val_loader)\n","    return val_accuracy, val_loss"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"x95X1qF_2y2B","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":9,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def train_model(model, model_name, num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device):\n","    best_val_accuracy = 0\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = train_loop(model, device, optimizer, criterion, train_loader)\n","        train_losses.append(epoch_loss)\n","\n","        # validation\n","        val_accuracy, val_loss = evaluate_model(model,val_loader, criterion , device)\n","        val_losses.append(val_loss)\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Best Val Loss: {best_val_loss:.4f}')\n","\n","        # Learning rate scheduling\n","        scheduler.step(val_accuracy)\n","\n","        if val_loss <= best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","            # Save the best model\n","            # return model\n","            torch.save(model.state_dict(), f'{model_name}_model.pt')\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve >= patience:\n","                print('Early stopping!')\n","\n","                break\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n","    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.title(\"Training and Validation Loss\")\n","    plt.xticks(range(1, len(train_losses) + 1))\n","    plt.legend()\n","    plt.show()\n","\n","    return model"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"7k0-iDp72y2C","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":9,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["def test_model(model,device, test_loader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for sequences, labels in tqdm(test_loader):\n","            sequences = sequences.to(device)\n","            labels = labels.to(device)\n","            outputs = model(sequences)\n","            predicted = (outputs >= 0.5).long()\n","            correct += (predicted == labels.long()).sum().item()\n","            total += labels.size(0)\n","    test_accuracy = correct / total\n","    print(f'Accuracy Score on Test dataset: {test_accuracy:.4f}')"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"mJvWBNtH2y2C","executionInfo":{"status":"ok","timestamp":1730954203165,"user_tz":-480,"elapsed":9,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["# Create datasets for training, validation, and testing\n","train_dataset = SentimentDataset(train_raw['text'], train_raw['label'], word2idx)\n","val_dataset = SentimentDataset(validation_raw['text'], validation_raw['label'], word2idx)\n","test_dataset = SentimentDataset(test_raw['text'], test_raw['label'], word2idx)\n","\n","# Create DataLoaders for training, validation, and testing\n","batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"Icp9stQ02y2C"},"source":["### Part 2b: Report the accuracy score on the test set, as well as the accuracy score on the validation set for each epoch during training."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OTgLF_5x2y2D","executionInfo":{"status":"ok","timestamp":1730955308785,"user_tz":-480,"elapsed":1105628,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"521d8cf6-b830-4ab1-843e-af42fed2f822"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Training Loss: 0.6558, Validation Accuracy: 0.7120, Validation Loss: 0.6031, Best Val Loss: inf\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [00:59<00:00,  1.12it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/30], Training Loss: 0.5455, Validation Accuracy: 0.7233, Validation Loss: 0.5344, Best Val Loss: 0.6031\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:03<00:00,  1.05it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/30], Training Loss: 0.5072, Validation Accuracy: 0.7580, Validation Loss: 0.5235, Best Val Loss: 0.5344\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:07<00:00,  1.01s/it]\n","100%|██████████| 9/9 [00:03<00:00,  2.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/30], Training Loss: 0.4965, Validation Accuracy: 0.7458, Validation Loss: 0.5196, Best Val Loss: 0.5235\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.11it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/30], Training Loss: 0.4754, Validation Accuracy: 0.7542, Validation Loss: 0.5012, Best Val Loss: 0.5196\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.08it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/30], Training Loss: 0.4701, Validation Accuracy: 0.7448, Validation Loss: 0.5050, Best Val Loss: 0.5012\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.11it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/30], Training Loss: 0.4395, Validation Accuracy: 0.7645, Validation Loss: 0.4912, Best Val Loss: 0.5012\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/30], Training Loss: 0.4266, Validation Accuracy: 0.7720, Validation Loss: 0.4919, Best Val Loss: 0.4912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:03<00:00,  1.06it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/30], Training Loss: 0.4146, Validation Accuracy: 0.7720, Validation Loss: 0.4884, Best Val Loss: 0.4912\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.11it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/30], Training Loss: 0.4103, Validation Accuracy: 0.7617, Validation Loss: 0.4958, Best Val Loss: 0.4884\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/30], Training Loss: 0.3960, Validation Accuracy: 0.7533, Validation Loss: 0.4869, Best Val Loss: 0.4884\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/30], Training Loss: 0.3766, Validation Accuracy: 0.7795, Validation Loss: 0.4845, Best Val Loss: 0.4869\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:05<00:00,  1.02it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/30], Training Loss: 0.3670, Validation Accuracy: 0.7627, Validation Loss: 0.4887, Best Val Loss: 0.4845\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [14/30], Training Loss: 0.3587, Validation Accuracy: 0.7636, Validation Loss: 0.4918, Best Val Loss: 0.4845\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:03<00:00,  1.06it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [15/30], Training Loss: 0.3506, Validation Accuracy: 0.7702, Validation Loss: 0.5111, Best Val Loss: 0.4845\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [16/30], Training Loss: 0.3316, Validation Accuracy: 0.7674, Validation Loss: 0.5048, Best Val Loss: 0.4845\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:00<00:00,  1.11it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [17/30], Training Loss: 0.3249, Validation Accuracy: 0.7598, Validation Loss: 0.5311, Best Val Loss: 0.4845\n","Early stopping!\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["SentimentRNN(\n","  (embedding): Embedding(18031, 100)\n","  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (fc1): Linear(in_features=256, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":40}],"source":["# Instantiate the model\n","rnn_model = SentimentRNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","rnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(rnn_model, 'rnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AqH1n7GV2y2D","executionInfo":{"status":"ok","timestamp":1730955312198,"user_tz":-480,"elapsed":3422,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}},"outputId":"90d20dcc-1d1e-4852-cd23-12f407b28dd3"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-41-f7d08250d7d6>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  rnn_model.load_state_dict(torch.load('rnn_model.pt'))\n","100%|██████████| 9/9 [00:03<00:00,  2.57it/s]"]},{"output_type":"stream","name":"stdout","text":["Accuracy Score on Test dataset: 0.7777\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Load the best model and evaluate on the test set\n","rnn_model.load_state_dict(torch.load('rnn_model.pt'))\n","test_model(rnn_model,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"sW15PBlR2y2D"},"source":["## Part 2c) Methods used in deriving the final sentence representation"]},{"cell_type":"markdown","metadata":{"id":"i6SDqckq2y2D"},"source":["### Part 2c.1) Method 1: Max-Pooling"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"nZZwnSLA2y2E","executionInfo":{"status":"ok","timestamp":1730955312198,"user_tz":-480,"elapsed":5,"user":{"displayName":"zhengxuan","userId":"07051997263670623787"}}},"outputs":[],"source":["class SentimentRNNWithMaxPooling(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNNWithMaxPooling, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, _ = self.lstm(embeds)\n","\n","        max_pool, _ = torch.max(lstm_out, dim = 1) #Considers the maximum output of all analyzed LSTM layers to determine the final result.\n","\n","        out = self.dropout(max_pool)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b1vFDD9u2y2I","outputId":"fa027418-22e9-470a-ae03-78943bf3b4a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Training Loss: 0.6549, Validation Accuracy: 0.6970, Validation Loss: 0.6112, Best Val Loss: inf\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.08it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/30], Training Loss: 0.5677, Validation Accuracy: 0.7083, Validation Loss: 0.5418, Best Val Loss: 0.6112\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.08it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/30], Training Loss: 0.5290, Validation Accuracy: 0.7448, Validation Loss: 0.5311, Best Val Loss: 0.5418\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [4/30], Training Loss: 0.5101, Validation Accuracy: 0.7664, Validation Loss: 0.5073, Best Val Loss: 0.5311\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:07<00:00,  1.00s/it]\n","100%|██████████| 9/9 [00:02<00:00,  3.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [5/30], Training Loss: 0.4819, Validation Accuracy: 0.7448, Validation Loss: 0.4930, Best Val Loss: 0.5073\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.08it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [6/30], Training Loss: 0.4657, Validation Accuracy: 0.7608, Validation Loss: 0.4926, Best Val Loss: 0.4930\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.10it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [7/30], Training Loss: 0.4560, Validation Accuracy: 0.7692, Validation Loss: 0.4899, Best Val Loss: 0.4926\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.08it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [8/30], Training Loss: 0.4465, Validation Accuracy: 0.7580, Validation Loss: 0.4847, Best Val Loss: 0.4899\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:04<00:00,  1.05it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [9/30], Training Loss: 0.4339, Validation Accuracy: 0.7702, Validation Loss: 0.4769, Best Val Loss: 0.4847\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [10/30], Training Loss: 0.4182, Validation Accuracy: 0.7833, Validation Loss: 0.4970, Best Val Loss: 0.4769\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [11/30], Training Loss: 0.4063, Validation Accuracy: 0.7730, Validation Loss: 0.4762, Best Val Loss: 0.4769\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.07it/s]\n","100%|██████████| 9/9 [00:02<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [12/30], Training Loss: 0.3963, Validation Accuracy: 0.7645, Validation Loss: 0.4715, Best Val Loss: 0.4762\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.07it/s]\n","100%|██████████| 9/9 [00:03<00:00,  2.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [13/30], Training Loss: 0.3769, Validation Accuracy: 0.7758, Validation Loss: 0.4855, Best Val Loss: 0.4715\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 67/67 [01:02<00:00,  1.07it/s]\n"," 78%|███████▊  | 7/9 [00:02<00:00,  3.32it/s]"]}],"source":["# Instantiate the model\n","rnn_model = SentimentRNNWithMaxPooling(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","rnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(rnn_model, 'maxpooling', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss_fm0eH2y2J"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","rnn_model.load_state_dict(torch.load('maxpooling_model.pt'))\n","test_model(rnn_model,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"gpscTeVQ2y2J"},"source":["### Part 2c.2) Method 2: Mean-Pooling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27jPppqE2y2K"},"outputs":[],"source":["class SentimentRNNWithAvgPooling(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNNWithAvgPooling, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, _ = self.lstm(embeds)\n","\n","        mean_pool= lstm_out.mean(dim = 1) #Considers the mean output of all analyzed LSTM layers to determine the final result.\n","\n","        out = self.dropout(mean_pool)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du_tjVEZ2y2K"},"outputs":[],"source":["# Instantiate the model\n","rnn_model = SentimentRNNWithAvgPooling(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","rnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(rnn_model, 'meanpooling', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzP3ghYc2y2K"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","rnn_model.load_state_dict(torch.load('meanpooling_model.pt'))\n","test_model(rnn_model,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"WeTfo-sc2y2K"},"source":["### Part 2c.3) Method 3: Simple Attention Mechanism"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RsrW8S2D2y2L"},"outputs":[],"source":["class SimpleAttention(nn.Module):\n","  def __init__(self, hidden_size):\n","      super(SimpleAttention, self).__init__()\n","      self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n","      self.bias = nn.Parameter(torch.Tensor(hidden_size))\n","\n","  def forward(self, hidden_states):\n","      # hidden_states: [batch_size, seq_len, hidden_size]\n","      batch_size, seq_len, hidden_size = hidden_states.size()\n","\n","      # Calculate attention weights\n","      weights = torch.bmm(hidden_states, self.weight.unsqueeze(0).repeat(batch_size, 1, 1))\n","      weights = torch.tanh(weights + self.bias.unsqueeze(0).unsqueeze(1).repeat(batch_size, seq_len, 1))\n","      weights = torch.softmax(weights, dim=1)  # Normalize weights\n","\n","      # Weight the hidden states\n","      weighted_hidden_states = hidden_states * weights\n","\n","      # Aggregate the weighted hidden states\n","      context_vector = torch.sum(weighted_hidden_states, dim=1)  # Sum across time steps\n","\n","      return context_vector\n","\n","\n","# Define the RNN model using pre-trained embeddings\n","class SentimentRNNWithSimpleAttention(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNNWithSimpleAttention, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","        self.attention = SimpleAttention(hidden_size=256)\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Apply Attention Mechanism\n","        context_vector = self.attention(lstm_out)\n","\n","        context_vector = self.dropout(context_vector)\n","        context_vector = self.relu(self.fc1(context_vector))\n","        context_vector = self.dropout(context_vector)\n","        context_vector = torch.sigmoid(self.fc2(context_vector))\n","        return context_vector.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6D9kAjXM2y2L"},"outputs":[],"source":["# Instantiate the model\n","rnn_model = SentimentRNNWithSimpleAttention(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","rnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(rnn_model, 'simpleattention', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOvb0FTj2y2L"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","rnn_model.load_state_dict(torch.load('simpleattention_model.pt'))\n","test_model(rnn_model,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"8gqQUsd12y2L"},"source":["### Part 2c.4) Method 4: Self-Attention Mechanism"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AorsgjsA2y2L"},"outputs":[],"source":["class SelfAttention(nn.Module):\n","  def __init__(self, hidden_size):\n","      super(SelfAttention, self).__init__()\n","      self.query = nn.Linear(hidden_size, hidden_size)  # Linear layer for query\n","      self.key = nn.Linear(hidden_size, hidden_size)    # Linear layer for key\n","      self.value = nn.Linear(hidden_size, hidden_size)  # Linear layer for value\n","\n","  def forward(self, hidden_states):\n","      # hidden_states: [batch_size, seq_len, hidden_size]\n","      batch_size, seq_len, hidden_size = hidden_states.size()\n","\n","      # Calculate vectors\n","      Q = self.query(hidden_states)\n","      K = self.key(hidden_states)\n","      V = self.value(hidden_states)\n","\n","      # Calculate attention weights\n","      attention_weights = torch.bmm(Q, K.transpose(1, 2)) / (hidden_size ** 0.5)\n","      attention_weights = torch.softmax(attention_weights, dim=-1)\n","\n","      # Weight the value vectors by attention weights\n","      weighted_values = torch.bmm(attention_weights, V)\n","\n","      return weighted_values\n","\n","class SimpleAttention(nn.Module):\n","  def __init__(self, hidden_size):\n","      super(SimpleAttention, self).__init__()\n","      self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n","      self.bias = nn.Parameter(torch.Tensor(hidden_size))\n","\n","  def forward(self, hidden_states):\n","      # hidden_states: [batch_size, seq_len, hidden_size]\n","      batch_size, seq_len, hidden_size = hidden_states.size()\n","\n","      # Calculate attention weights\n","      weights = torch.bmm(hidden_states, self.weight.unsqueeze(0).repeat(batch_size, 1, 1))\n","      weights = torch.tanh(weights + self.bias.unsqueeze(0).unsqueeze(1).repeat(batch_size, seq_len, 1))\n","      weights = torch.softmax(weights, dim=1)  # Normalize weights\n","\n","      # Weight the hidden states\n","      weighted_hidden_states = hidden_states * weights\n","\n","      # Aggregate the weighted hidden states\n","      context_vector = torch.sum(weighted_hidden_states, dim=1)  # Sum across time steps\n","\n","      return context_vector\n","\n","\n","# Define the RNN model using pre-trained embeddings\n","class SentimentRNNWithSelfAttention(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNNWithSelfAttention, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.relu = nn.ReLU()\n","        self.self_attention = SelfAttention(hidden_size=256)\n","        self.simple_attention = SimpleAttention(hidden_size=256)\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        # Apply self-attention\n","        self_attention_out = self.self_attention(lstm_out)\n","\n","        # Apply Simple attention\n","        context_vector = self.simple_attention(self_attention_out)\n","\n","        context_vector = self.dropout(context_vector)\n","        context_vector = self.relu(self.fc1(context_vector))\n","        context_vector = self.dropout(context_vector)\n","        context_vector = torch.sigmoid(self.fc2(context_vector))\n","        return context_vector.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sHUS-uFg2y2M"},"outputs":[],"source":["# Instantiate the model\n","rnn_model = SentimentRNNWithSelfAttention(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","rnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(rnn_model, 'selfattention', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaYNVqJ02y2M"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","rnn_model.load_state_dict(torch.load('selfattention_model.pt'))\n","test_model(rnn_model,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"HXbEuOSP2y2M"},"source":["# Part 3 Enhancement"]},{"cell_type":"markdown","metadata":{"id":"mvuVnF6B2y2M"},"source":["## Part 3.1 Word Embeddings Updated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpRBBFBJ2y2N"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9URDiNyA2y2N"},"outputs":[],"source":["# Define the RNN model using pre-trained embeddings\n","class SentimentRNNUpdatedEmbeddings(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentRNNUpdatedEmbeddings, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze= False)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=False, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(256, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        out, _ = torch.max(lstm_out, dim=1)\n","        # Concatenate the final hidden states from both directions\n","        # out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjxZGoyz2y2N"},"outputs":[],"source":["# Instantiate the model\n","sentimentRNNUpdatedEmbeddings = SentimentRNNUpdatedEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","sentimentRNNUpdatedEmbeddings.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(sentimentRNNUpdatedEmbeddings.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(sentimentRNNUpdatedEmbeddings,'rnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vje6Soyg2y2O"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","sentimentRNNUpdatedEmbeddings.load_state_dict(torch.load('rnn_model.pt'))\n","test_model(sentimentRNNUpdatedEmbeddings,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"38ENqGU32y2O"},"source":["## Part 3.2 OOV Strategy 1: Random Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZkXtXD62y2O"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Tjmzlq22y2O"},"outputs":[],"source":["# Instantiate the model\n","sentimentRNNUpdatedEmbeddings = SentimentRNNUpdatedEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","sentimentRNNUpdatedEmbeddings.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(sentimentRNNUpdatedEmbeddings.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(sentimentRNNUpdatedEmbeddings,'rnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AWzXXVro2y2O"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","sentimentRNNUpdatedEmbeddings.load_state_dict(torch.load('rnn_model.pt'))\n","test_model(sentimentRNNUpdatedEmbeddings,device, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWH9krRc2y2O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NX9y6mq12y2P"},"source":["## Part 3.2 OOV Strategy 2: Byte-pair Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfsM8vPR2y2P"},"outputs":[],"source":["\n","# Load the Rotten Tomatoes dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Prepare the text data for BPE training\n","train_texts = [entry['text'] for entry in train_raw]\n","\n","# Step 3: Initialize and Train the BPE Tokenizer\n","# Define a ByteLevelBPETokenizer instance\n","tokenizer = ByteLevelBPETokenizer()\n","\n","# Train the tokenizer on the training texts\n","tokenizer.train_from_iterator(train_texts, vocab_size=30000, min_frequency=2)\n","\n","# Save the trained tokenizer (optional)\n","tokenizer.save_model(\".\", \"bpe_tokenizer\")\n","\n","# Step 4: Apply the Trained BPE Tokenizer to Your Datasets\n","# Tokenize a sample from the training set\n","train_tokenized = [tokenizer.encode(text).tokens for text in train_texts]\n","\n","# Similarly tokenize the validation and test sets\n","validation_texts = [entry['text'] for entry in validation_raw]\n","validation_tokenized = [tokenizer.encode(text).tokens for text in validation_texts]\n","\n","test_texts = [entry['text'] for entry in test_raw]\n","test_tokenized = [tokenizer.encode(text).tokens for text in test_texts]\n","\n","# Print a sample tokenized text\n","print(\"Tokenized sample from training set:\", train_tokenized[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KfASaFc2y2P"},"outputs":[],"source":["def load_glove_embeddings(filepath):\n","    embeddings_index = {}\n","    with open(filepath, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","def get_subword_embedding(subwords, glove_embeddings, embedding_dim=100):\n","    vectors = []\n","    for subword in subwords:\n","        if subword in glove_embeddings:\n","            vectors.append(glove_embeddings[subword])\n","    if not vectors:\n","        # Handle the case where no subwords have embeddings\n","        return np.random.uniform(-0.05, 0.05, (embedding_dim,))\n","    # Average or sum the vectors\n","    return np.mean(vectors, axis=0)  # For averaging\n","    # return np.sum(vectors, axis=0)   # For summation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcqpuhrK2y2P"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","glove_embeddings = load_glove_embeddings('/glove.6B.100d.txt')\n","tokenizer = ByteLevelBPETokenizer.from_file(\"bpe_tokenizer-vocab.json\", \"bpe_tokenizer-merges.txt\")\n","\n","for word in word2idx:\n","    idx = word2idx[word]\n","    if word in glove_embeddings:\n","        vector = glove_embeddings[word]\n","        embedding_matrix[idx] = vector\n","\n","    else:\n","        subwords = tokenizer.encode(word).tokens\n","        # print(subwords)\n","        vector = get_subword_embedding(subwords=subwords, glove_embeddings=glove_embeddings)\n","        embedding_matrix[idx] = vector\n","\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uc_CGW4P2y2P"},"outputs":[],"source":["# Instantiate the model\n","sentimentRNNUpdatedEmbeddings = SentimentRNNUpdatedEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","sentimentRNNUpdatedEmbeddings.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(sentimentRNNUpdatedEmbeddings.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(sentimentRNNUpdatedEmbeddings,'rnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI4j2gs62y2Q"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","sentimentRNNUpdatedEmbeddings.load_state_dict(torch.load('rnn_model.pt'))\n","test_model(sentimentRNNUpdatedEmbeddings,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"9ruTYj-A2y2Q"},"source":["## Part 3.2 OOV Strategy 3: Backoff and Interpolation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSmtZJVY2y2Q"},"outputs":[],"source":["dataset = load_dataset(\"rotten_tomatoes\")\n","train_dataset = dataset['train']\n","vocabulary = set()\n","\n","for text in train_dataset['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Print the size of the vocabulary\n","print(\"The size of the vocabulary is:\", len(vocabulary))\n","\n","glove_vocab = set()\n","glove_embeddings = {}\n","embedding_dim = 100\n","with open('/glove.6B.100d.txt', 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        glove_vocab.add(word)\n","        glove_embeddings[word] = np.asarray(values[1:], dtype='float32')\n","\n","# Identify OOV words\n","oov_words = vocabulary - glove_vocab\n","\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for idx, word in enumerate(vocabulary)}\n","\n","# Initialize the embedding matrix for known words\n","embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n","for word in glove_vocab:\n","    if word in word2idx:  # Check if the word is in word2idx\n","        idx = word2idx[word]\n","        embedding_matrix[idx] = glove_embeddings[word]\n","\n","\n","embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n","for word in glove_vocab:\n","    if word in word2idx:  # Check if the word is in word2idx\n","        idx = word2idx[word]\n","        embedding_matrix[idx] = glove_embeddings[word]\n","\n","\n","# Create n-grams from training data\n","def create_ngrams(texts, n):\n","    ngram_counts = defaultdict(int)\n","    for entry in texts:  # Adjusted to iterate over each entry (dictionary)\n","        text = entry['text']  # Access the text with the appropriate key\n","        tokens = word_tokenize(text.lower())\n","        ngrams = zip(*[tokens[i:] for i in range(n)])  # Create n-grams\n","        for ngram in ngrams:\n","            ngram_counts[' '.join(ngram)] += 1\n","    return ngram_counts\n","\n","# Create bigrams and trigrams\n","bigram_counts = create_ngrams(train_dataset, 2)  # Use train_raw directly\n","trigram_counts = create_ngrams(train_dataset, 3)  # Use train_raw directly\n","\n","# Backoff and interpolation parameters\n","lambda1 = 0.8  # weight for unigrams\n","lambda2 = 0.15  # weight for bigrams\n","lambda3 = 0.05  # weight for trigrams\n","\n","# Define a function for backoff and interpolation handling OOV words\n","def get_ngram_embedding(word, embedding_matrix, word2idx, bigram_counts, trigram_counts):\n","    if word in word2idx:\n","        return embedding_matrix[word2idx[word]]\n","    else:\n","        # Initialize embeddings for interpolation\n","        unigram_embedding = np.zeros(embedding_matrix.shape[1])\n","        bigram_embedding = np.zeros(embedding_matrix.shape[1])\n","        trigram_embedding = np.zeros(embedding_matrix.shape[1])\n","\n","        # Check trigram context\n","        trigram_context = [ngram for ngram in trigram_counts if word in ngram.split()]\n","        if trigram_context:\n","            context_ngram = trigram_context[0].split()\n","            trigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in context_ngram if w in word2idx], axis=0)\n","\n","        # Check bigram context\n","        bigram_context = [ngram for ngram in bigram_counts if word in ngram.split()]\n","        if bigram_context:\n","            context_ngram = bigram_context[0].split()\n","            bigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in context_ngram if w in word2idx], axis=0)\n","\n","        # Calculate unigram embedding (mean of known embeddings, if available)\n","        known_words = [w for w in word2idx if w in vocabulary]\n","        if known_words:\n","            unigram_embedding = np.mean([embedding_matrix[word2idx[w]] for w in known_words if word2idx[w] < len(embedding_matrix)], axis=0)\n","\n","        # Apply interpolation\n","        final_embedding = (lambda1 * unigram_embedding) + (lambda2 * bigram_embedding) + (lambda3 * trigram_embedding)\n","        return final_embedding\n","\n","# Handle OOV words using n-gram embeddings\n","for oov_word in oov_words:\n","    if oov_word in word2idx:  # Ensure the OOV word exists in word2idx\n","        idx = word2idx[oov_word]\n","        embedding_matrix[idx] = get_ngram_embedding(oov_word, embedding_matrix, word2idx, bigram_counts, trigram_counts)\n","\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)\n","\n","# Now, the embedding_matrix contains GloVe embeddings for known words and n-gram based embeddings for OOV words\n","print(\"Embedding matrix shape:\", embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp3cXyxi2y2R"},"outputs":[],"source":["# Instantiate the model\n","sentimentRNNUpdatedEmbeddings = SentimentRNNUpdatedEmbeddings(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","sentimentRNNUpdatedEmbeddings.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(sentimentRNNUpdatedEmbeddings.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(sentimentRNNUpdatedEmbeddings,'rnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZzg0X642y2R"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","sentimentRNNUpdatedEmbeddings.load_state_dict(torch.load('rnn_model.pt'))\n","test_model(sentimentRNNUpdatedEmbeddings,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"s9kApaFb2y2R"},"source":["## Part 3.3 biLSTM model and a biGRU model"]},{"cell_type":"markdown","metadata":{"id":"7Uu_9_yt2y2R"},"source":["## Part 3.3.1 biLSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lV80_P5n2y2S"},"outputs":[],"source":["class SentimentLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze= False)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        out, _ = torch.max(lstm_out, dim=1)\n","        # Concatenate the final hidden states from both directions\n","        # out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Esil1HVs2y2T"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd_DD3lW2y2T"},"outputs":[],"source":["# Instantiate the model\n","biLSTM = SentimentLSTM(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","biLSTM.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(biLSTM.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(biLSTM,'bilstm', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRRqW0z42y2U"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","biLSTM.load_state_dict(torch.load('bilstm_model.pt'))\n","test_model(biLSTM,device, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"kJlDS2su2y2U"},"source":["## Part 3.3.2 biGRU Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-pI-WTN2y2U"},"outputs":[],"source":["class SentimentBiGRU(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentBiGRU, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n","        self.gru = nn.GRU(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=2, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        gru_out, h_n = self.gru(embeds)\n","\n","        # Concatenate the final hidden states from both directions\n","        out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmf2z4S42y2U"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0KW_yGN2y2U"},"outputs":[],"source":["# Instantiate the model\n","biGRU_model = SentimentBiGRU(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","biGRU_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(biGRU_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(biGRU_model,'bigru', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHuSka5y2y2V"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","biGRU_model.load_state_dict(torch.load('bigru_model.pt'))\n","test_model(biGRU_model,device, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RE_abbN02y2V"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C5WXKAa32y2V"},"source":["## Part 3.4 Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvwMOCrI2y2V"},"outputs":[],"source":["class SentimentCNN(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentCNN, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n","\n","        # 1D Convolution layers to capture local features\n","        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv1d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(128, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","\n","        # Other layers\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Input: (batch_size, sequence_length)\n","        embeds = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n","\n","        # Convolution expects input with shape (batch_size, embedding_dim, sequence_length)\n","        embeds = embeds.permute(0, 2, 1)\n","\n","        # Apply convolution layers and ReLU activation\n","        conv_out = self.relu(self.conv1(embeds))  # (batch_size, 256, sequence_length)\n","        conv_out = self.relu(self.conv2(conv_out))  # (batch_size, 128, sequence_length)\n","\n","        # Global Max Pooling (to reduce the sequence length dimension)\n","        pooled_out = torch.max(conv_out, dim=2)[0]  # (batch_size, 128)\n","\n","        # Fully connected layers with dropout and ReLU\n","        out = self.dropout(pooled_out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","\n","        # Sigmoid activation for binary classification\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlSsgjrU2y2W"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix\n","embedding_dim = 100  # Dimensionality of GloVe embeddings\n","vocab_size = len(word2idx)\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings (ensure 'glove.6B.100d.txt' is in your working directory)\n","glove_path = '/glove.6B.100d.txt'  # Update the path if necessary\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim + 1:\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx] = vector\n","\n","# Set the embedding for '<pad>' token to zeros\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3iDKZ6W2y2X"},"outputs":[],"source":["# Instantiate the model\n","cnn_model = SentimentCNN(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","cnn_model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(cnn_model,'cnn', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_Z0WCFo2y2X"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","cnn_model.load_state_dict(torch.load('cnn_model.pt'))\n","test_model(cnn_model,device, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqV0SLY72y2X"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oxrO8wxo2y2X"},"source":["#"]},{"cell_type":"markdown","metadata":{"id":"Hj0b9IPc2y2X"},"source":["## Part 3.5 Improve Model"]},{"cell_type":"markdown","metadata":{"id":"wRVqUG9m2y2Y"},"source":["### Part 3.5.1 Improvement Strategy 1: BERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LcxtXkN2y2Y"},"outputs":[],"source":["nltk.download('punkt')\n","\n","# Set random seeds for reproducibility\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqTvRhz72y2Y"},"outputs":[],"source":["# Check device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBzaIzRs2y2Y"},"outputs":[],"source":["# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICfB6_4O2y2Y"},"outputs":[],"source":["# Initialize BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Define maximum sequence length\n","MAX_LEN = 128\n","\n","# Define the custom Dataset class for BERT\n","class BERTSentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","\n","        input_ids = encoding['input_ids'].flatten()\n","        attention_mask = encoding['attention_mask'].flatten()\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': torch.tensor(label, dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7rOcKtFb2y2Y"},"outputs":[],"source":["# Create datasets for training, validation, and testing\n","train_dataset = BERTSentimentDataset(\n","    texts=train_raw['text'],\n","    labels=train_raw['label'],\n","    tokenizer=tokenizer,\n","    max_len=MAX_LEN\n",")\n","\n","val_dataset = BERTSentimentDataset(\n","    texts=validation_raw['text'],\n","    labels=validation_raw['label'],\n","    tokenizer=tokenizer,\n","    max_len=MAX_LEN\n",")\n","\n","test_dataset = BERTSentimentDataset(\n","    texts=test_raw['text'],\n","    labels=test_raw['label'],\n","    tokenizer=tokenizer,\n","    max_len=MAX_LEN\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVA7Lr602y2Y"},"outputs":[],"source":["# Define the DataLoaders\n","batch_size = 32\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Load pre-trained BERT model for sequence classification\n","model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=1,  # Binary classification\n","    output_attentions=False,\n","    output_hidden_states=False\n",")\n","\n","model.to(device)\n","\n","# Define optimizer with weight decay\n","optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","\n","# Total number of training steps\n","total_steps = len(train_loader) * 10  # Assuming 10 epochs\n","\n","# Create the learning rate scheduler with warm-up\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","# Define loss function\n","criterion = torch.nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEGeBYhe2y2Z"},"outputs":[],"source":["# Function to calculate metrics\n","def bert_evaluate(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits.squeeze()\n","            preds = torch.sigmoid(logits)\n","            predictions.extend(preds.cpu().numpy())\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    # Convert predictions to binary\n","    binary_preds = [1 if p >= 0.5 else 0 for p in predictions]\n","\n","    acc = accuracy_score(true_labels, binary_preds)\n","    f1 = f1_score(true_labels, binary_preds)\n","\n","    return acc, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6sQOz7t2y2Z"},"outputs":[],"source":["# Training loop with validation and early stopping\n","num_epochs = 10\n","patience = 3\n","best_val_f1 = 0\n","epochs_no_improve = 0\n","best_model_path = 'best_bert_model.pt'\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits.squeeze()\n","\n","        loss = criterion(logits, labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","\n","    # Validation\n","    val_accuracy, val_f1 = bert_evaluate(model, val_loader)\n","\n","    print(f'Epoch {epoch+1}/{num_epochs}')\n","    print(f'Training Loss: {avg_train_loss:.4f}')\n","    print(f'Validation Accuracy: {val_accuracy:.4f}')\n","    print(f'Validation F1 Score: {val_f1:.4f}')\n","\n","    # Check for improvement\n","    if val_f1 > best_val_f1:\n","        best_val_f1 = val_f1\n","        epochs_no_improve = 0\n","        torch.save(model.state_dict(), best_model_path)\n","        print('Validation F1 improved, saving model.')\n","    else:\n","        epochs_no_improve += 1\n","        print(f'No improvement in validation F1 for {epochs_no_improve} epoch(s).')\n","        if epochs_no_improve >= patience:\n","            print('Early stopping triggered!')\n","            break\n","\n","# Load the best model\n","model.load_state_dict(torch.load(best_model_path))\n","model.to(device)\n","\n","# Evaluate on test set\n","test_accuracy, test_f1 = bert_evaluate(model, test_loader)\n","print(f'\\nTest Accuracy: {test_accuracy:.4f}')\n","print(f'Test F1 Score: {test_f1:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"cF5a0f9O2y2Z"},"source":["### Part 3.5.2 Improvement Strategy 2: Modified GloVe Embeddings with Sentiment Scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzopHx7J2y2f"},"outputs":[],"source":["afinn = Afinn()\n","\n","# Function to fetch sentiment score\n","def get_sentiment_score(word):\n","    return afinn.score(word)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ayz-Mik82y2f"},"outputs":[],"source":["nltk.download('punkt')\n","\n","# Load the dataset\n","dataset = load_dataset(\"rotten_tomatoes\")\n","train_raw = dataset['train']\n","validation_raw = dataset['validation']\n","test_raw = dataset['test']\n","\n","# Build the vocabulary from training data\n","vocabulary = set()\n","for text in train_raw['text']:\n","    tokens = word_tokenize(text.lower())\n","    vocabulary.update(tokens)\n","\n","# Add special tokens\n","vocabulary.add('<unk>')\n","vocabulary.add('<pad>')\n","\n","# Create mappings between words and indices\n","word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx2word = {idx: word for word, idx in word2idx.items()}\n","\n","# Initialize the embedding matrix with an extra dimension for sentiment scores\n","embedding_dim = 100 + 1  # Original dimensionality of GloVe embeddings + 1 for sentiment score\n","vocab_size = len(word2idx)\n","# embedding_matrix = np.zeros((vocab_size, embedding_dim))  # Initialize with zeros\n","embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))\n","\n","# Load GloVe embeddings and include sentiment scores\n","glove_path = '/glove.6B.100d.txt'  # Make sure the path is correct\n","with open(glove_path, 'r', encoding='utf8') as f:\n","    for line in f:\n","        values = line.strip().split()\n","        if len(values) == embedding_dim:  # This should be just the embedding dimensions without sentiment score\n","            word = values[0]\n","            vector = np.asarray(values[1:], dtype='float32')\n","            if word in word2idx:\n","                idx = word2idx[word]\n","                embedding_matrix[idx, :-1] = vector  # Exclude the last dimension for sentiment score\n","                embedding_matrix[idx, -1] = get_sentiment_score(word)  # Add sentiment score in the last dimension\n","\n","# Set the embedding for '<pad>' token to zeros (including sentiment score)\n","pad_idx = word2idx['<pad>']\n","embedding_matrix[pad_idx] = np.zeros(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVdk3pxJ2y2f"},"outputs":[],"source":["# Create datasets for training, validation, and testing\n","train_dataset = SentimentDataset(train_raw['text'], train_raw['label'], word2idx)\n","val_dataset = SentimentDataset(validation_raw['text'], validation_raw['label'], word2idx)\n","test_dataset = SentimentDataset(test_raw['text'], test_raw['label'], word2idx)\n","\n","# Create DataLoaders for training, validation, and testing\n","batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59b2ahbZ2y2f"},"outputs":[],"source":["class SentimentLSTM(nn.Module):\n","    def __init__(self, embedding_matrix):\n","        super(SentimentLSTM, self).__init__()\n","        vocab_size, embedding_dim = embedding_matrix.shape\n","        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze= False)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_size=256, batch_first=True, bidirectional=True, num_layers=1, dropout=0.5)\n","        self.fc1 = nn.Linear(512, 128)\n","        self.fc2 = nn.Linear(128, 1)\n","        self.dropout = nn.Dropout(0.3)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, (h_n, c_n) = self.lstm(embeds)\n","\n","        out, _ = torch.max(lstm_out, dim=1)\n","        # Concatenate the final hidden states from both directions\n","        # out = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n","\n","        out = self.dropout(out)\n","        out = self.relu(self.fc1(out))\n","        out = self.dropout(out)\n","        out = torch.sigmoid(self.fc2(out))\n","        return out.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj_m169X2y2g"},"outputs":[],"source":["# Instantiate the model\n","biLSTM = SentimentLSTM(embedding_matrix)\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","biLSTM.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(biLSTM.parameters(), lr=0.001, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5, verbose=True)\n","\n","# Training loop with validation and early stopping\n","num_epochs = 30\n","patience = 5  # Early stopping patience\n","\n","train_model(biLSTM,'bilstm', num_epochs, optimizer, scheduler, criterion, patience, train_loader, val_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6ebiAf32y2g"},"outputs":[],"source":["# Load the best model and evaluate on the test set\n","biLSTM.load_state_dict(torch.load('bilstm_model.pt'))\n","test_model(biLSTM,device, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGFtHerY2y2g"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"nlp2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b9a20ae5237548e6a80abb860bac6323":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b80014742c5e42dab47b98a2b9b94fe5","IPY_MODEL_142a8e61f9b348fc95608e2bc16cc66c","IPY_MODEL_0e5d36ca14d540ceba753ede89b9a06a"],"layout":"IPY_MODEL_c87930e8a376406c84b5fb5f07c4862e"}},"b80014742c5e42dab47b98a2b9b94fe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e182b43be96042bb8bf5b58171331266","placeholder":"​","style":"IPY_MODEL_d2dd6e28f4414dc98880da0eaa36d60b","value":"README.md: 100%"}},"142a8e61f9b348fc95608e2bc16cc66c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db29269049944175afddb83bee682d62","max":7457,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65f9112e474f49f48272d2d3cd9c4ab4","value":7457}},"0e5d36ca14d540ceba753ede89b9a06a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4328e6389a8243cb9e1d5fe4b5559124","placeholder":"​","style":"IPY_MODEL_1758b9a77ef04110b7dec1aa0d0b4907","value":" 7.46k/7.46k [00:00&lt;00:00, 408kB/s]"}},"c87930e8a376406c84b5fb5f07c4862e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e182b43be96042bb8bf5b58171331266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2dd6e28f4414dc98880da0eaa36d60b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db29269049944175afddb83bee682d62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65f9112e474f49f48272d2d3cd9c4ab4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4328e6389a8243cb9e1d5fe4b5559124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1758b9a77ef04110b7dec1aa0d0b4907":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6913f74db4944ead821c2b7d82407a2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2fbc2ee839a24c049c8090ba65968ba8","IPY_MODEL_56af98a93bf14140a7904ba946d359ac","IPY_MODEL_45833cc283d3452a80b36fd4bcbcaa24"],"layout":"IPY_MODEL_d2a54c522b574b54a3b39428d48652d2"}},"2fbc2ee839a24c049c8090ba65968ba8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_044ada3fb50f49f2b494ce515b39f7ef","placeholder":"​","style":"IPY_MODEL_081f2af61989433d8ada5474f1a36519","value":"train.parquet: 100%"}},"56af98a93bf14140a7904ba946d359ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0ddd09733fc4863befb570409a72a73","max":698845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d4a16f91e5a4824bc9e4bea806e9280","value":698845}},"45833cc283d3452a80b36fd4bcbcaa24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_053f958c93b1456697b339829664ade8","placeholder":"​","style":"IPY_MODEL_f1f23bce20bf4f029733749e63b8822e","value":" 699k/699k [00:00&lt;00:00, 4.91MB/s]"}},"d2a54c522b574b54a3b39428d48652d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044ada3fb50f49f2b494ce515b39f7ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"081f2af61989433d8ada5474f1a36519":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0ddd09733fc4863befb570409a72a73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d4a16f91e5a4824bc9e4bea806e9280":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"053f958c93b1456697b339829664ade8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1f23bce20bf4f029733749e63b8822e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65b8b8cd83ca48b68fe9a6a76bab560a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5dbc27ea6224e03a4eb58874f843767","IPY_MODEL_ed9c200a935a48cbb402a1847591b0a4","IPY_MODEL_a483d2b1b36545fa8ebbb95bd346a6df"],"layout":"IPY_MODEL_323c0159c41947c681a2c41cc95f8fd0"}},"a5dbc27ea6224e03a4eb58874f843767":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db1370de113f4a2881f6807d1b543ce0","placeholder":"​","style":"IPY_MODEL_a8f77847effd4dfea7615a85f202f094","value":"validation.parquet: 100%"}},"ed9c200a935a48cbb402a1847591b0a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a8fb22104db4678926a639c6ea5f150","max":90001,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57edbfb3cda44e59bae8d10dfbc2fced","value":90001}},"a483d2b1b36545fa8ebbb95bd346a6df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0587627f55a84e039157191f35bb848b","placeholder":"​","style":"IPY_MODEL_e98aeb97e8934272964f352e40193b75","value":" 90.0k/90.0k [00:00&lt;00:00, 5.62MB/s]"}},"323c0159c41947c681a2c41cc95f8fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db1370de113f4a2881f6807d1b543ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8f77847effd4dfea7615a85f202f094":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a8fb22104db4678926a639c6ea5f150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57edbfb3cda44e59bae8d10dfbc2fced":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0587627f55a84e039157191f35bb848b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e98aeb97e8934272964f352e40193b75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"067ee71d667d4f9e81c2abc086dae57c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6845c31839a48109903d3e492f65be0","IPY_MODEL_b23b67ea7970468fa1ea1a3f238a3fb3","IPY_MODEL_e72110ba54774890ae0fc79646a4bcb7"],"layout":"IPY_MODEL_e62649bb5706440abd81204a1fbc386a"}},"e6845c31839a48109903d3e492f65be0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d7dbe1140744abdbe758ace4a7dbe4d","placeholder":"​","style":"IPY_MODEL_301e4bf39051476fb81f4655ce2b0f50","value":"test.parquet: 100%"}},"b23b67ea7970468fa1ea1a3f238a3fb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65181e38ac0140a1b34d9cc90e9b008d","max":92206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5c26fb416e5432583a9d1f8ce56f030","value":92206}},"e72110ba54774890ae0fc79646a4bcb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6595fbebf3f643a7ab704e13bd472b3c","placeholder":"​","style":"IPY_MODEL_d9183cbed6624a4ab1fbfcd643684139","value":" 92.2k/92.2k [00:00&lt;00:00, 4.85MB/s]"}},"e62649bb5706440abd81204a1fbc386a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d7dbe1140744abdbe758ace4a7dbe4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"301e4bf39051476fb81f4655ce2b0f50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65181e38ac0140a1b34d9cc90e9b008d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5c26fb416e5432583a9d1f8ce56f030":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6595fbebf3f643a7ab704e13bd472b3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9183cbed6624a4ab1fbfcd643684139":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"195cc754a96b4bdda1cad4ff56e2d5a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92345bc213cb48d3b3ec7749c1d0f7f6","IPY_MODEL_77ddc22da66d4eccb610938a2a2e0322","IPY_MODEL_13372279ded9411e98acb56fede07b48"],"layout":"IPY_MODEL_f23db805c4dc46cbbdffc1876e8003de"}},"92345bc213cb48d3b3ec7749c1d0f7f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71f0228e70234679ae791d67136c4935","placeholder":"​","style":"IPY_MODEL_08edc4a257c54be68897d5348c1b7b73","value":"Generating train split: 100%"}},"77ddc22da66d4eccb610938a2a2e0322":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e250d3ba2db84d908794c2258ddf0597","max":8530,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7a76d5bf7c14ca7afbd88ff20c72d4f","value":8530}},"13372279ded9411e98acb56fede07b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3540ae7204ad41debee359a772d5249a","placeholder":"​","style":"IPY_MODEL_b036265172a84241bc4efc665abc3960","value":" 8530/8530 [00:00&lt;00:00, 109399.67 examples/s]"}},"f23db805c4dc46cbbdffc1876e8003de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71f0228e70234679ae791d67136c4935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08edc4a257c54be68897d5348c1b7b73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e250d3ba2db84d908794c2258ddf0597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a76d5bf7c14ca7afbd88ff20c72d4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3540ae7204ad41debee359a772d5249a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b036265172a84241bc4efc665abc3960":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d874d461224742dab38b09532a57a76e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b698d676ad946a78b01e0a4f2923545","IPY_MODEL_b2d44d3bb1da464a87c1ad9e50ce509b","IPY_MODEL_5279cc42c7a84a2fb00059f6018d1425"],"layout":"IPY_MODEL_f908e6ceea0b4ccd8e03ec750bddf7fc"}},"6b698d676ad946a78b01e0a4f2923545":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efe29be66846448295b1518447c0058b","placeholder":"​","style":"IPY_MODEL_a2924aef5f1b433a8252a4855c018c6d","value":"Generating validation split: 100%"}},"b2d44d3bb1da464a87c1ad9e50ce509b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e13c1f2957d4b70ad4d9d32b9903167","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80c97b139ff84b36bb0139c45151046c","value":1066}},"5279cc42c7a84a2fb00059f6018d1425":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb94b9b807374752905d47796718ddfa","placeholder":"​","style":"IPY_MODEL_a65147124eb345eb8ba927a738d67f5d","value":" 1066/1066 [00:00&lt;00:00, 35977.41 examples/s]"}},"f908e6ceea0b4ccd8e03ec750bddf7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efe29be66846448295b1518447c0058b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2924aef5f1b433a8252a4855c018c6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e13c1f2957d4b70ad4d9d32b9903167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80c97b139ff84b36bb0139c45151046c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb94b9b807374752905d47796718ddfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a65147124eb345eb8ba927a738d67f5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"801946a64d0442648420a6b8415b32f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_547c8db506994fda96f48fa935ed2b68","IPY_MODEL_83a9cad4aff4448699735eb133b298ab","IPY_MODEL_fa3c66466ddc4b35925d853fb13be101"],"layout":"IPY_MODEL_4f128c6d43b64575978c0249b9c03807"}},"547c8db506994fda96f48fa935ed2b68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dae08162aa54c049312ced2903a178a","placeholder":"​","style":"IPY_MODEL_711728eb780946d38d5196c2493d9c9e","value":"Generating test split: 100%"}},"83a9cad4aff4448699735eb133b298ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee6e94bb8d56497cb63435dbccff0aa7","max":1066,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9acf7fd111f4488a88ca5979f2ac4f2","value":1066}},"fa3c66466ddc4b35925d853fb13be101":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f7de7f3a86442d2ab0dc617585b34a7","placeholder":"​","style":"IPY_MODEL_399556dfd68f4f6b86426a5db89166b7","value":" 1066/1066 [00:00&lt;00:00, 20339.86 examples/s]"}},"4f128c6d43b64575978c0249b9c03807":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dae08162aa54c049312ced2903a178a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711728eb780946d38d5196c2493d9c9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee6e94bb8d56497cb63435dbccff0aa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9acf7fd111f4488a88ca5979f2ac4f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f7de7f3a86442d2ab0dc617585b34a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"399556dfd68f4f6b86426a5db89166b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}